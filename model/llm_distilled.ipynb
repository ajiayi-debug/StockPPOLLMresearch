{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85888fb3",
   "metadata": {},
   "source": [
    "# Distillation of gpt 4o mini into llama 3.1 8B for Stock Price Prediction: LLM-Based Forecasting (with Risk-Aware PPO Adjustment)\n",
    "\n",
    "This notebook runs inference using llama 3.1 8B with distilled inference from teacher model gpt 4o mini (using seq knowledge distillation)\n",
    "\n",
    "## Framework Overview:\n",
    "1. **Stage 1**: LLM-based stock price prediction using historical data, technical indicators, and sentiment analysis\n",
    "2. **Stage 2**: Risk-aware PPO adjustment incorporating VaR and CVaR to refine predictions (ablation from paper)\n",
    "\n",
    "## Dataset:\n",
    "- Training, validation, and test data from finetune_paper directory\n",
    "- Stocks: AAPL, HSBC, PEP, 0700.HK (Tencent), 7203.T (Toyota)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3dacb7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245fb057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: yfinance in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (0.2.66)\n",
      "Requirement already satisfied: pandas in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: numpy in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: nltk in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: requests in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 5)) (2.32.5)\n",
      "Requirement already satisfied: feedparser in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (6.0.12)\n",
      "Requirement already satisfied: transformers in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 7)) (4.56.2)\n",
      "Requirement already satisfied: torch in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 8)) (2.8.0)\n",
      "Requirement already satisfied: xgboost in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 11)) (3.0.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 12)) (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: accelerate in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 14)) (1.10.1)\n",
      "Requirement already satisfied: peft in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 15)) (0.17.1)\n",
      "Requirement already satisfied: safetensors in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 16)) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 17)) (0.35.0)\n",
      "Requirement already satisfied: openai in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 18)) (2.5.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (4.13.5)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (6.32.1)\n",
      "Requirement already satisfied: websockets>=13.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from yfinance->-r ../requirements.txt (line 1)) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from pandas->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from pandas->-r ../requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: click in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from nltk->-r ../requirements.txt (line 4)) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from nltk->-r ../requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from nltk->-r ../requirements.txt (line 4)) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from nltk->-r ../requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->-r ../requirements.txt (line 5)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->-r ../requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->-r ../requirements.txt (line 5)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->-r ../requirements.txt (line 5)) (2025.8.3)\n",
      "Requirement already satisfied: sgmllib3k in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from feedparser->-r ../requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: filelock in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers->-r ../requirements.txt (line 7)) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers->-r ../requirements.txt (line 7)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers->-r ../requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers->-r ../requirements.txt (line 7)) (0.22.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from huggingface_hub->-r ../requirements.txt (line 17)) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from huggingface_hub->-r ../requirements.txt (line 17)) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from huggingface_hub->-r ../requirements.txt (line 17)) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch->-r ../requirements.txt (line 8)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch->-r ../requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch->-r ../requirements.txt (line 8)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch->-r ../requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: scipy in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from xgboost->-r ../requirements.txt (line 11)) (1.16.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from scikit-learn->-r ../requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 13)) (3.2.5)\n",
      "Requirement already satisfied: psutil in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from accelerate->-r ../requirements.txt (line 14)) (7.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 18)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 18)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 18)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 18)) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 18)) (2.11.9)\n",
      "Requirement already satisfied: sniffio in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 18)) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r ../requirements.txt (line 18)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ../requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r ../requirements.txt (line 18)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r ../requirements.txt (line 18)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r ../requirements.txt (line 18)) (0.4.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance->-r ../requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from curl_cffi>=0.7->yfinance->-r ../requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance->-r ../requirements.txt (line 1)) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch->-r ../requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from jinja2->torch->-r ../requirements.txt (line 8)) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c500d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (4.56.2)\n",
      "Requirement already satisfied: accelerate in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (0.42.0)\n",
      "Requirement already satisfied: torch in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: psutil in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: scipy in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from bitsandbytes) (1.16.2)\n",
      "Requirement already satisfied: setuptools in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/angjiayi/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face packages (run once if using local Llama)\n",
    "!pip install transformers accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8568c9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard library\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# HTTP requests for HF endpoint\n",
    "import requests\n",
    "\n",
    "# # Machine Learning\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e4277",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Dedicated Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256f7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face Dedicated Endpoint configured!\n",
      "   Endpoint: https://xr4if8jpbpi8884c.us-east-1.aws.endpoints.huggingface.cloud\n",
      "   Model: Llama 3.1 8B Instruct\n",
      "   Max Tokens: 1024\n",
      "   Temperature: 0.0\n",
      "   Rate limits: UNLIMITED! üéâ\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# LLM Configuration\n",
    "MAX_TOKENS = 1024\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "# Hugging Face Dedicated Endpoint\n",
    "HF_ENDPOINT_URL = \"https://xr4if8jpbpi8884c.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "# Get HF token\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file. Get token from: https://huggingface.co/settings/tokens\")\n",
    "\n",
    "print(f\"‚úÖ Hugging Face Dedicated Endpoint configured!\")\n",
    "print(f\"   Endpoint: {HF_ENDPOINT_URL}\")\n",
    "print(f\"   Model: Llama 3.1 8B Instruct\")\n",
    "print(f\"   Max Tokens: {MAX_TOKENS}\")\n",
    "print(f\"   Temperature: {TEMPERATURE}\")\n",
    "print(f\"   Rate limits: UNLIMITED! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb19231",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97c9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 2477\n",
      "\n",
      "All labels shape: (12418, 16)\n",
      "\n",
      "Stocks in dataset: ['AAPL' 'HSBC' '0700.HK' 'PEP' '7203.T']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load train, val, test data\n",
    "# train_data = load_jsonl('../finetune_paper/train.jsonl')\n",
    "# val_data = load_jsonl('../finetune_paper/val.jsonl')\n",
    "test_data = load_jsonl('../finetune_paper/test.jsonl')\n",
    "\n",
    "# Load supervised labels\n",
    "all_labels = pd.read_csv('../finetune_paper/all_supervised_price_labels.csv')\n",
    "\n",
    "# print(f\"Training samples: {len(train_data)}\")\n",
    "# print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nAll labels shape: {all_labels.shape}\")\n",
    "print(f\"\\nStocks in dataset: {all_labels['ticker'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5df0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "Prompt (first 500 chars): You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-justified, considering multiple financial factors.\n",
      "\n",
      "‚Ä¢ Predicted Stock Price: The forecasted close price for the next trading day.\n",
      "‚Ä¢ Price Movement Likelihood: The likelihood of the predicted stock pric...\n",
      "\n",
      "Response: {\"predicted_close\": 32.68000030517578, \"likelihood\": 0.9, \"justification\": \"n/a\"}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sample supervised labels:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>MACD_hist</th>\n",
       "      <th>BB_width_20_2</th>\n",
       "      <th>headline_count</th>\n",
       "      <th>sent_compound_mean</th>\n",
       "      <th>titles_joined</th>\n",
       "      <th>next_close</th>\n",
       "      <th>confidence_proxy</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.159062</td>\n",
       "      <td>27.234398</td>\n",
       "      <td>13.536208</td>\n",
       "      <td>-0.075335</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>-0.059645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.079550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.765558</td>\n",
       "      <td>46.231136</td>\n",
       "      <td>4.645025</td>\n",
       "      <td>-0.465578</td>\n",
       "      <td>-0.348537</td>\n",
       "      <td>-0.117041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.308567</td>\n",
       "      <td>Which London business pays the highest busines...</td>\n",
       "      <td>45.360001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.078837</td>\n",
       "      <td>109.846862</td>\n",
       "      <td>68.406756</td>\n",
       "      <td>3.231975</td>\n",
       "      <td>2.607665</td>\n",
       "      <td>0.624309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.388344</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0700.HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.059458</td>\n",
       "      <td>95.400737</td>\n",
       "      <td>36.546590</td>\n",
       "      <td>0.658721</td>\n",
       "      <td>0.411460</td>\n",
       "      <td>0.247261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>Audrey P. \"Pep\" Landry Obituary January 16, 20...</td>\n",
       "      <td>97.510002</td>\n",
       "      <td>0.5</td>\n",
       "      <td>PEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-19 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.126453</td>\n",
       "      <td>110.109194</td>\n",
       "      <td>70.079261</td>\n",
       "      <td>3.017259</td>\n",
       "      <td>2.689584</td>\n",
       "      <td>0.327675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>WeChat apologizes for showering Chinese users ...</td>\n",
       "      <td>114.402382</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0700.HK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  SMA_20  SMA_50      EMA_12      EMA_26  \\\n",
       "0  2015-01-16 00:00:00+00:00     NaN     NaN   27.159062   27.234398   \n",
       "1  2015-01-16 00:00:00+00:00     NaN     NaN   45.765558   46.231136   \n",
       "2  2015-01-16 00:00:00+00:00     NaN     NaN  113.078837  109.846862   \n",
       "3  2015-01-16 00:00:00+00:00     NaN     NaN   96.059458   95.400737   \n",
       "4  2015-01-19 00:00:00+00:00     NaN     NaN  113.126453  110.109194   \n",
       "\n",
       "      RSI_14      MACD  MACD_signal  MACD_hist  BB_width_20_2  headline_count  \\\n",
       "0  13.536208 -0.075335    -0.015690  -0.059645            NaN             4.0   \n",
       "1   4.645025 -0.465578    -0.348537  -0.117041            NaN             6.0   \n",
       "2  68.406756  3.231975     2.607665   0.624309            NaN             1.0   \n",
       "3  36.546590  0.658721     0.411460   0.247261            NaN            10.0   \n",
       "4  70.079261  3.017259     2.689584   0.327675            NaN             1.0   \n",
       "\n",
       "   sent_compound_mean                                      titles_joined  \\\n",
       "0           -0.079550                                                NaN   \n",
       "1            0.308567  Which London business pays the highest busines...   \n",
       "2            0.000000                                                NaN   \n",
       "3            0.082980  Audrey P. \"Pep\" Landry Obituary January 16, 20...   \n",
       "4            0.361200  WeChat apologizes for showering Chinese users ...   \n",
       "\n",
       "   next_close  confidence_proxy   ticker  \n",
       "0   27.180000               0.5     AAPL  \n",
       "1   45.360001               0.9     HSBC  \n",
       "2  113.388344               0.5  0700.HK  \n",
       "3   97.510002               0.5      PEP  \n",
       "4  114.402382               0.5  0700.HK  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample data\n",
    "print(\"Sample training data:\")\n",
    "print(f\"Prompt (first 500 chars): {test_data[0]['prompt'][:500]}...\")\n",
    "print(f\"\\nResponse: {test_data[0]['response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Sample supervised labels:\")\n",
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef5039f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed test data shape: (2477, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_close</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>32.680000</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0700.HK</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>342.870056</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PEP</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>178.970001</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>126.360001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7203.T</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1807.500000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker        date  predicted_close  likelihood\n",
       "0     HSBC  2023-01-03        32.680000         0.9\n",
       "1  0700.HK  2023-01-03       342.870056         0.5\n",
       "2      PEP  2023-01-03       178.970001         0.9\n",
       "3     AAPL  2023-01-03       126.360001         0.5\n",
       "4   7203.T  2023-01-04      1807.500000         0.7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse test data for evaluation\n",
    "POSITIVE_JUSTIFICATION_KEYWORDS = {\n",
    "    \"increase\", \"growth\", \"upward\", \"bullish\", \"positive\", \"gain\", \"improve\", \"strength\", \"rally\", \"optimistic\"\n",
    "}\n",
    "NEGATIVE_JUSTIFICATION_KEYWORDS = {\n",
    "    \"decrease\", \"decline\", \"downward\", \"bearish\", \"negative\", \"loss\", \"drop\", \"weakness\", \"sell\", \"pessimistic\"\n",
    "}\n",
    "RISK_JUSTIFICATION_KEYWORDS = {\n",
    "    \"volatility\", \"volatile\", \"risk\", \"uncertain\", \"uncertainty\", \"caution\", \"concern\", \"warning\", \"downside\"\n",
    "}\n",
    "\n",
    "def parse_prompt_data(prompt_text):\n",
    "    \"\"\"Extract key information from prompt\"\"\"\n",
    "    lines = prompt_text.split('\\n')\n",
    "    data = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        if 'TICKER:' in line:\n",
    "            data['ticker'] = line.split('TICKER:')[1].strip()\n",
    "        elif 'DATE:' in line:\n",
    "            data['date'] = line.split('DATE:')[1].strip()\n",
    "        elif 'RECENT CLOSING PRICES' in line:\n",
    "            prices_line = lines[lines.index(line) + 1]\n",
    "            if prices_line.strip():\n",
    "                data['recent_prices'] = [float(p.strip()) for p in prices_line.split(',') if p.strip()]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def safe_float(value, default=0.0) -> float:\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return float(default)\n",
    "\n",
    "def extract_justification_features(justification: str) -> Dict[str, float]:\n",
    "    base = {\n",
    "        \"justification_pos_ratio\": 0.0,\n",
    "        \"justification_neg_ratio\": 0.0,\n",
    "        \"justification_risk_ratio\": 0.0,\n",
    "        \"justification_polarity\": 0.0,\n",
    "        \"justification_length\": 0.0,\n",
    "    }\n",
    "    if not justification:\n",
    "        return base.copy()\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", justification.lower())\n",
    "    token_count = max(len(tokens), 1)\n",
    "    pos_hits = sum(token in POSITIVE_JUSTIFICATION_KEYWORDS for token in tokens)\n",
    "    neg_hits = sum(token in NEGATIVE_JUSTIFICATION_KEYWORDS for token in tokens)\n",
    "    risk_hits = sum(token in RISK_JUSTIFICATION_KEYWORDS for token in tokens)\n",
    "    base.update({\n",
    "        \"justification_pos_ratio\": float(pos_hits / token_count),\n",
    "        \"justification_neg_ratio\": float(neg_hits / token_count),\n",
    "        \"justification_risk_ratio\": float(risk_hits / token_count),\n",
    "        \"justification_polarity\": float((pos_hits - neg_hits) / token_count),\n",
    "        \"justification_length\": float(np.log1p(token_count)),\n",
    "    })\n",
    "    return base\n",
    "\n",
    "# Parse test data\n",
    "test_parsed = []\n",
    "for item in test_data:\n",
    "    parsed = parse_prompt_data(item['prompt'])\n",
    "    response = json.loads(item['response'])\n",
    "    parsed['predicted_close'] = response['predicted_close']\n",
    "    parsed['likelihood'] = response['likelihood']\n",
    "    test_parsed.append(parsed)\n",
    "\n",
    "test_df = pd.DataFrame(test_parsed)\n",
    "print(f\"Parsed test data shape: {test_df.shape}\")\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b6114",
   "metadata": {},
   "source": [
    "## 4. Stage 1: LLM-Based Stock Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c62801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Hugging Face Dedicated Endpoint with a sample prediction...\n",
      "================================================================================\n",
      "Sample prompt:\n",
      "You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-justified, considering multiple financial factors.\n",
      "\n",
      "‚Ä¢ Predicted Stock Price: The forecasted close price for the next trading day.\n",
      "‚Ä¢ Price Movement Likelihood: The likelihood of the predicted stock price.\n",
      "‚Ä¢ Justification: Provide an explanation for the predicted stock price and the corresponding likelihood, considering the following:\n",
      "  - Historical market data (e.g., recent closing prices).\n",
      "  - Technical indicators (e.g., SMA, EMA, RSI, MACD, Bollinger Bands).\n",
      "  - Sentiment analysis (e.g., news sentiment, market sentiment).\n",
      "\n",
      "Please weigh these signals and justify the predicted stock price.\n",
      "\n",
      "TICKER: HSBC\n",
      "DATE: 2023-01-03\n",
      "\n",
      "RECENT CLOSING PRICES (most recent last): 31.0700, 31.0300, 31.2100, 31.1600, 31.6300\n",
      "\n",
      "TECHNICAL INDICATORS:\n",
      "SMA_20=30.614999866485597, SMA_50=29.027199935913085,\n",
      "EMA_12=30.909996996764857, EMA_26=30.33953977919106,\n",
      "RSI_14=70.01903430263613, MACD=0.5704572175737965, MACD_signal=0.5609859479794548, MACD_hist=0.0094712695943417,\n",
      "BB_width_20_2=0.0625429400529223\n",
      "\n",
      "SENTIMENT AGGREGATES:\n",
      "headline_count=4.0, sent_compound_mean=0.072325\n",
      "\n",
      "HEADLINES (concise):\n",
      "BP, Unilever, and HSBC have failed to properly exit Russia after Ukraine war, new report warns - City AM | Rising interest rates to boost HSBC profit amid uncertain economic outlook - South China Morning Post | HSBC connects virtual and real world with seasonal campaign - Marketing-Interactive | John Lloyd leaves HSBC - Asset Servicing Times\n",
      "\n",
      "Return STRICT JSON with keys:\n",
      "- predicted_close (float, next-day close price),\n",
      "- likelihood (float in [0,1]),\n",
      "- justification (string, 1‚Äì2 sentences).\n",
      "JSON:...\n",
      "\n",
      "‚è∞ Generating prediction...\n",
      "\n",
      "‚è±Ô∏è Inference time: 3.15 seconds\n",
      "\n",
      "HF Endpoint Prediction Result:\n",
      "{\n",
      "  \"predicted_close\": 31.5,\n",
      "  \"likelihood\": 0.75,\n",
      "  \"justification\": \"The recent closing prices show a slight upward trend, supported by a strong RSI indicating overbought conditions, while positive sentiment from headlines about HSBC's performance suggests investor confidence. Additionally, the MACD is above its signal line, reinforcing bullish momentum.\"\n",
      "}\n",
      "\n",
      "Actual Target Price: 32.68000030517578\n",
      "\n",
      "‚úÖ HF Dedicated Endpoint is working!\n",
      "üí° Speed: ~3.1s per prediction\n",
      "üí° No rate limits - run unlimited predictions!\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è Inference time: 3.15 seconds\n",
      "\n",
      "HF Endpoint Prediction Result:\n",
      "{\n",
      "  \"predicted_close\": 31.5,\n",
      "  \"likelihood\": 0.75,\n",
      "  \"justification\": \"The recent closing prices show a slight upward trend, supported by a strong RSI indicating overbought conditions, while positive sentiment from headlines about HSBC's performance suggests investor confidence. Additionally, the MACD is above its signal line, reinforcing bullish momentum.\"\n",
      "}\n",
      "\n",
      "Actual Target Price: 32.68000030517578\n",
      "\n",
      "‚úÖ HF Dedicated Endpoint is working!\n",
      "üí° Speed: ~3.1s per prediction\n",
      "üí° No rate limits - run unlimited predictions!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def llm_predict_stock_price(prompt: str) -> Dict:\n",
    "    \"\"\"Use Hugging Face Dedicated Endpoint to predict stock price\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {hf_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": MAX_TOKENS,\n",
    "                \"temperature\": TEMPERATURE if TEMPERATURE > 0 else 0.1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            HF_ENDPOINT_URL,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"HF Endpoint Error: {response.status_code} - {response.text}\")\n",
    "            return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"API Error: {response.status_code}\"}\n",
    "        \n",
    "        result_data = response.json()\n",
    "        \n",
    "        # Extract generated text\n",
    "        if isinstance(result_data, list) and len(result_data) > 0:\n",
    "            content = result_data[0].get('generated_text', '')\n",
    "        elif isinstance(result_data, dict):\n",
    "            content = result_data.get('generated_text', result_data.get('text', ''))\n",
    "        else:\n",
    "            content = str(result_data)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        if '{' in content and '}' in content:\n",
    "            json_start = content.index('{')\n",
    "            json_end = content.rindex('}') + 1\n",
    "            json_str = content[json_start:json_end]\n",
    "            \n",
    "            try:\n",
    "                result = json.loads(json_str)\n",
    "                \n",
    "                # Validate required fields\n",
    "                if 'predicted_close' not in result:\n",
    "                    result['predicted_close'] = None\n",
    "                if 'likelihood' not in result:\n",
    "                    result['likelihood'] = 0.5\n",
    "                if 'justification' not in result:\n",
    "                    result['justification'] = ''\n",
    "                    \n",
    "                return result\n",
    "            except json.JSONDecodeError as je:\n",
    "                print(f\"JSON parse error, attempting manual extraction: {je}\")\n",
    "                \n",
    "                # Try to extract values manually\n",
    "                pred_match = re.search(r'\"predicted_close\"\\s*:\\s*([0-9.]+)', json_str)\n",
    "                likelihood_match = re.search(r'\"likelihood\"\\s*:\\s*([0-9.]+)', json_str)\n",
    "                \n",
    "                if pred_match:\n",
    "                    return {\n",
    "                        \"predicted_close\": float(pred_match.group(1)),\n",
    "                        \"likelihood\": float(likelihood_match.group(1)) if likelihood_match else 0.5,\n",
    "                        \"justification\": \"Manually extracted from malformed JSON\"\n",
    "                    }\n",
    "                else:\n",
    "                    return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"JSON parse error: {str(je)}\"}\n",
    "        else:\n",
    "            return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": \"No JSON found in response\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in HF endpoint prediction: {e}\")\n",
    "        return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": str(e)}\n",
    "\n",
    "# Test HF Endpoint\n",
    "print(\"üß™ Testing Hugging Face Dedicated Endpoint with a sample prediction...\")\n",
    "print(\"=\"*80)\n",
    "sample_prompt = test_data[0]['prompt']\n",
    "print(\"Sample prompt:\")\n",
    "print(sample_prompt + \"...\\n\")\n",
    "\n",
    "print(\"‚è∞ Generating prediction...\")\n",
    "start_time = time.time()\n",
    "\n",
    "llm_result = llm_predict_stock_price(sample_prompt)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Inference time: {elapsed:.2f} seconds\")\n",
    "print(\"\\nHF Endpoint Prediction Result:\")\n",
    "print(json.dumps(llm_result, indent=2))\n",
    "\n",
    "actual_response = json.loads(test_data[0]['response'])\n",
    "print(f\"\\nActual Target Price: {actual_response['predicted_close']}\")\n",
    "print(f\"\\n‚úÖ HF Dedicated Endpoint is working!\")\n",
    "print(f\"üí° Speed: ~{elapsed:.1f}s per prediction\")\n",
    "print(f\"üí° No rate limits - run unlimited predictions!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cba71",
   "metadata": {},
   "source": [
    "### 4.1 Run LLM Inference on Test Data\n",
    "\n",
    "Generate predictions for test data (used for final evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ad03246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh LLM predictions...\n",
      "Generating LLM predictions for 2477 samples...\n",
      "This may take a while due to API rate limits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   1%|          | 17/2477 [01:02<2:47:38,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 370)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   2%|‚ñè         | 50/2477 [03:01<2:18:31,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   4%|‚ñç         | 100/2477 [06:00<2:38:45,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   6%|‚ñå         | 150/2477 [08:59<2:05:38,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   7%|‚ñã         | 183/2477 [10:56<2:58:22,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 459)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   8%|‚ñä         | 200/2477 [11:57<2:23:33,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:   9%|‚ñâ         | 217/2477 [13:08<3:12:36,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 402)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  10%|‚ñà         | 250/2477 [15:12<2:12:39,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  12%|‚ñà‚ñè        | 300/2477 [18:11<2:11:53,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  13%|‚ñà‚ñé        | 330/2477 [19:59<2:40:03,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  14%|‚ñà‚ñç        | 350/2477 [21:12<2:05:51,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  15%|‚ñà‚ñå        | 381/2477 [23:05<2:39:49,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  16%|‚ñà‚ñå        | 400/2477 [24:10<1:59:58,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  18%|‚ñà‚ñä        | 450/2477 [27:04<1:50:27,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  20%|‚ñà‚ñà        | 500/2477 [30:00<1:49:52,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  22%|‚ñà‚ñà‚ñè       | 550/2477 [33:03<1:55:59,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  23%|‚ñà‚ñà‚ñé       | 564/2477 [33:52<2:12:04,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  24%|‚ñà‚ñà‚ñç       | 600/2477 [36:00<1:45:41,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  26%|‚ñà‚ñà‚ñå       | 650/2477 [38:53<1:44:47,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  26%|‚ñà‚ñà‚ñã       | 652/2477 [39:03<2:16:00,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 493)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  27%|‚ñà‚ñà‚ñã       | 664/2477 [39:46<1:57:49,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 354)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  28%|‚ñà‚ñà‚ñä       | 700/2477 [41:50<1:37:24,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  30%|‚ñà‚ñà‚ñà       | 750/2477 [44:45<1:46:04,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  32%|‚ñà‚ñà‚ñà‚ñè      | 790/2477 [47:17<1:58:04,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 369)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  32%|‚ñà‚ñà‚ñà‚ñè      | 800/2477 [47:54<1:44:07,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  34%|‚ñà‚ñà‚ñà‚ñç      | 850/2477 [50:54<1:43:28,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  36%|‚ñà‚ñà‚ñà‚ñå      | 894/2477 [53:31<2:20:23,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 3 column 1 (char 393)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  36%|‚ñà‚ñà‚ñà‚ñã      | 900/2477 [53:52<1:40:20,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  38%|‚ñà‚ñà‚ñà‚ñä      | 950/2477 [56:59<1:39:53,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  40%|‚ñà‚ñà‚ñà‚ñà      | 1000/2477 [1:00:01<1:29:54,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1050/2477 [1:03:09<1:37:52,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1100/2477 [1:06:01<1:17:45,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1150/2477 [1:08:51<1:15:11,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1200/2477 [1:11:42<1:12:24,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1250/2477 [1:14:40<1:09:08,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1300/2477 [1:17:37<1:10:33,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1350/2477 [1:20:27<1:04:10,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1400/2477 [1:23:28<1:03:44,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1450/2477 [1:26:29<1:05:11,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1500/2477 [1:29:30<56:14,  3.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1509/2477 [1:30:03<1:08:50,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 368)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1550/2477 [1:32:23<52:45,  3.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1600/2477 [1:35:21<50:41,  3.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1650/2477 [1:38:11<46:17,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1700/2477 [1:41:12<44:45,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1713/2477 [1:41:59<49:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 323)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1750/2477 [1:44:04<42:07,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1800/2477 [1:47:02<38:51,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1850/2477 [1:49:53<36:37,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1900/2477 [1:52:51<33:17,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1950/2477 [1:55:54<33:55,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 1950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2000/2477 [1:58:49<30:14,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2050/2477 [2:01:41<24:16,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2100/2477 [2:04:44<21:57,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2150/2477 [2:07:46<19:41,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2189/2477 [2:09:59<21:06,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 437)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2200/2477 [2:10:37<16:15,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2250/2477 [2:13:34<12:21,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 2300/2477 [2:16:33<11:02,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2350/2477 [2:19:28<07:14,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 2400/2477 [2:22:30<04:27,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 2403/2477 [2:22:43<05:21,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 457)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 2410/2477 [2:23:10<04:57,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2450/2477 [2:25:29<01:35,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at index 2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2468/2477 [2:26:32<00:35,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON parse error, attempting manual extraction: Extra data: line 7 column 1 (char 391)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2477/2477 [2:27:07<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM predictions completed: 2477 samples\n",
      "Checkpoint saved to: ../results/llm_predictions_justification_checkpoint.json\n",
      "Sample predictions:\n",
      "    ticker  llm_prediction  actual_price\n",
      "0     HSBC           31.50     32.680000\n",
      "1  0700.HK          322.00    342.870056\n",
      "2      PEP          178.25    178.970001\n",
      "3     AAPL          124.50    126.360001\n",
      "4   7203.T         1805.00   1807.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run LLM predictions on test data with checkpointing\n",
    "import time\n",
    "\n",
    "# Checkpoint file to save progress\n",
    "checkpoint_file = '../results/llm_predictions_justification_checkpoint.json'\n",
    "\n",
    "# Load existing checkpoint if available\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(f\"Loading existing checkpoint from {checkpoint_file}\")\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    llm_predictions = checkpoint['predictions']\n",
    "    actual_prices = checkpoint['actual_prices']\n",
    "    llm_results = checkpoint.get('llm_results', [])\n",
    "    start_idx = checkpoint['last_idx'] + 1\n",
    "    print(f\"Resuming from index {start_idx}/{len(test_data)}\")\n",
    "else:\n",
    "    llm_predictions = []\n",
    "    actual_prices = []\n",
    "    llm_results = []\n",
    "    start_idx = 0\n",
    "    print(\"Starting fresh LLM predictions...\")\n",
    "\n",
    "# Run LLM predictions with rate limiting and checkpointing\n",
    "print(f\"Generating LLM predictions for {len(test_data)} samples...\")\n",
    "print(\"This may take a while due to API rate limits...\")\n",
    "\n",
    "for idx in tqdm(range(start_idx, len(test_data)), desc=\"LLM Inference\"):\n",
    "    item = test_data[idx]\n",
    "    \n",
    "    try:\n",
    "        # Get LLM prediction\n",
    "        llm_result = llm_predict_stock_price(item['prompt'])\n",
    "        \n",
    "        # Store full LLM result\n",
    "        llm_results.append(llm_result)\n",
    "        \n",
    "        # Extract prediction\n",
    "        if llm_result['predicted_close'] is not None:\n",
    "            llm_predictions.append(llm_result['predicted_close'])\n",
    "        else:\n",
    "            # Fallback: use a simple baseline if LLM fails\n",
    "            response = json.loads(item['response'])\n",
    "            llm_predictions.append(response['predicted_close'])\n",
    "        \n",
    "        # Get actual price from response\n",
    "        response = json.loads(item['response'])\n",
    "        actual_prices.append(response['predicted_close'])\n",
    "        \n",
    "        # Small delay to avoid rate limiting (adjust based on your API limits)\n",
    "        #time.sleep(0.5)\n",
    "\n",
    "        # Checkpoint every 50 samples\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            checkpoint = {\n",
    "                'predictions': llm_predictions,\n",
    "                'actual_prices': actual_prices,\n",
    "                'llm_results': llm_results,\n",
    "                'last_idx': idx\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            print(f\"Checkpoint saved at index {idx + 1}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if 'rate_limit' in error_msg.lower() or 'too many requests' in error_msg.lower():\n",
    "            print(f\"‚ùå RATE LIMIT HIT at index {idx}!\")\n",
    "            print(f\"Saving checkpoint and stopping execution...\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'predictions': llm_predictions,\n",
    "                'actual_prices': actual_prices,\n",
    "                'llm_results': llm_results,\n",
    "                'last_idx': idx - 1\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            \n",
    "            print(f\"‚úÖ Checkpoint saved to: {checkpoint_file}\")\n",
    "            print(f\"üìä Progress: {idx}/{len(test_data)} samples completed\")\n",
    "            print(f\"üí° Run this cell again to resume from where you left off.\")\n",
    "            break  # Stop execution\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error at index {idx}: {error_msg}\")\n",
    "            # Store error result\n",
    "            error_result = {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"Error: {error_msg}\"}\n",
    "            llm_results.append(error_result)\n",
    "            # Use fallback\n",
    "            response = json.loads(item['response'])\n",
    "            llm_predictions.append(response['predicted_close'])\n",
    "            actual_prices.append(response['predicted_close'])\n",
    "\n",
    "# Final save\n",
    "checkpoint = {\n",
    "    'predictions': llm_predictions,\n",
    "    'actual_prices': actual_prices,\n",
    "    'llm_results': llm_results,\n",
    "    'last_idx': len(llm_predictions) - 1,\n",
    "    'completed': len(llm_predictions) == len(test_data)\n",
    "}\n",
    "with open(checkpoint_file, 'w') as f:\n",
    "    json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "# Merge with test_df\n",
    "test_df['llm_prediction'] = llm_predictions\n",
    "test_df['actual_price'] = actual_prices\n",
    "\n",
    "if len(llm_results) == len(test_df):\n",
    "    justifications = []\n",
    "    likelihoods = []\n",
    "    feature_rows = []\n",
    "    for res in llm_results:\n",
    "        res = res if isinstance(res, dict) else {}\n",
    "        justification = res.get('justification', '')\n",
    "        justifications.append(justification)\n",
    "        likelihoods.append(safe_float(res.get('likelihood'), 0.5))\n",
    "        feature_rows.append(extract_justification_features(justification))\n",
    "else:\n",
    "    justifications = [''] * len(test_df)\n",
    "    likelihoods = [0.5] * len(test_df)\n",
    "    feature_rows = [extract_justification_features('') for _ in range(len(test_df))]\n",
    "\n",
    "if feature_rows:\n",
    "    feature_keys = list(feature_rows[0].keys())\n",
    "else:\n",
    "    feature_keys = list(extract_justification_features('').keys())\n",
    "\n",
    "test_df['llm_justification'] = justifications\n",
    "test_df['llm_likelihood'] = likelihoods\n",
    "for key in feature_keys:\n",
    "    test_df[key] = [row[key] for row in feature_rows]\n",
    "\n",
    "if len(llm_predictions) == len(test_data):\n",
    "    print(f\"‚úÖ LLM predictions completed: {len(llm_predictions)} samples\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Partial completion: {len(llm_predictions)}/{len(test_data)} samples\")\n",
    "print(f\"Checkpoint saved to: {checkpoint_file}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(test_df[['ticker', 'llm_prediction', 'actual_price']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6e7a4",
   "metadata": {},
   "source": [
    "### 4.4 Check for Failed Predictions in Checkpoints\n",
    "\n",
    "Before training PPO, let's verify all predictions succeeded and fix any failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1e8b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHECKING ALL CHECKPOINT FILES FOR FAILED PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìä TEST CHECKPOINT ANALYSIS\n",
      "================================================================================\n",
      "Total predictions: 2477\n",
      "Failed predictions: 0\n",
      "Success rate: 100.00%\n",
      "\n",
      "‚úÖ All predictions successful!\n",
      "\n",
      "================================================================================\n",
      "üìà OVERALL SUMMARY\n",
      "================================================================================\n",
      "Test:       0/2477 failed\n",
      "\n",
      "Total failed: 0/2477 (0.00%)\n",
      "\n",
      "üí° If any predictions failed, run the next cell to fix them.\n"
     ]
    }
   ],
   "source": [
    "# Check for failed predictions in all checkpoint files\n",
    "import json\n",
    "import os\n",
    "\n",
    "def check_failed_predictions(checkpoint_file, data_name):\n",
    "    \"\"\"Check for failed/None predictions in checkpoint\"\"\"\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        print(f\"‚ùå {data_name} checkpoint not found: {checkpoint_file}\")\n",
    "        return None\n",
    "    \n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    \n",
    "    predictions = checkpoint.get('predictions', [])\n",
    "    llm_results = checkpoint.get('llm_results', [])\n",
    "    \n",
    "    # Find indices with failed predictions\n",
    "    failed_indices = []\n",
    "    for idx, (pred, result) in enumerate(zip(predictions, llm_results)):\n",
    "        if pred is None or (isinstance(result, dict) and result.get('predicted_close') is None):\n",
    "            failed_indices.append(idx)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä {data_name.upper()} CHECKPOINT ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Failed predictions: {len(failed_indices)}\")\n",
    "    print(f\"Success rate: {((len(predictions) - len(failed_indices)) / len(predictions) * 100):.2f}%\")\n",
    "    \n",
    "    if failed_indices:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed prediction indices (first 20): {failed_indices[:20]}\")\n",
    "        if len(failed_indices) > 20:\n",
    "            print(f\"   ... and {len(failed_indices) - 20} more\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All predictions successful!\")\n",
    "    \n",
    "    return {\n",
    "        'checkpoint_file': checkpoint_file,\n",
    "        'total': len(predictions),\n",
    "        'failed': len(failed_indices),\n",
    "        'failed_indices': failed_indices,\n",
    "        'checkpoint': checkpoint\n",
    "    }\n",
    "\n",
    "# Check all three checkpoints\n",
    "print(\"üîç CHECKING ALL CHECKPOINT FILES FOR FAILED PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "test_check = check_failed_predictions(\n",
    "    '../results/llm_predictions_checkpoint.json', \n",
    "    'Test'\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìà OVERALL SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if test_check:\n",
    "    print(f\"Test:       {test_check['failed']}/{test_check['total']} failed\")\n",
    "\n",
    "total_failed = 0\n",
    "total_samples = 0\n",
    "\n",
    "if test_check:\n",
    "    total_failed += test_check['failed']\n",
    "    total_samples += test_check['total']\n",
    "\n",
    "print(f\"\\nTotal failed: {total_failed}/{total_samples} ({(total_failed/total_samples*100):.2f}%)\")\n",
    "print(f\"\\nüí° If any predictions failed, run the next cell to fix them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6689abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ ALL FAILED PREDICTIONS HAVE BEEN PROCESSED!\n",
      "================================================================================\n",
      "You can now proceed with PPO training.\n"
     ]
    }
   ],
   "source": [
    "# Re-run inference ONLY for failed predictions\n",
    "def fix_failed_predictions(checkpoint_info, original_data, data_name):\n",
    "    \"\"\"Re-run inference for failed predictions only\"\"\"\n",
    "    if not checkpoint_info or not checkpoint_info['failed_indices']:\n",
    "        print(f\"‚úÖ {data_name}: No failed predictions to fix!\")\n",
    "        return checkpoint_info['checkpoint']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîÑ FIXING FAILED PREDICTIONS FOR {data_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Failed predictions to fix: {len(checkpoint_info['failed_indices'])}\")\n",
    "    \n",
    "    checkpoint = checkpoint_info['checkpoint']\n",
    "    predictions = checkpoint['predictions']\n",
    "    actual_prices = checkpoint['actual_prices']\n",
    "    llm_results = checkpoint['llm_results']\n",
    "    \n",
    "    fixed_count = 0\n",
    "    still_failed = []\n",
    "    \n",
    "    for idx in tqdm(checkpoint_info['failed_indices'], desc=f\"Fixing {data_name}\"):\n",
    "        try:\n",
    "            item = original_data[idx]\n",
    "            \n",
    "            # Re-run LLM prediction\n",
    "            llm_result = llm_predict_stock_price(item['prompt'])\n",
    "            \n",
    "            # Update results\n",
    "            llm_results[idx] = llm_result\n",
    "            \n",
    "            # Update prediction\n",
    "            if llm_result['predicted_close'] is not None:\n",
    "                predictions[idx] = llm_result['predicted_close']\n",
    "                fixed_count += 1\n",
    "            else:\n",
    "                # Still failed, use fallback\n",
    "                response = json.loads(item['response'])\n",
    "                predictions[idx] = response['predicted_close']\n",
    "                still_failed.append(idx)\n",
    "            \n",
    "            # Small delay\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error fixing index {idx}: {e}\")\n",
    "            still_failed.append(idx)\n",
    "            # Use fallback\n",
    "            try:\n",
    "                response = json.loads(original_data[idx]['response'])\n",
    "                predictions[idx] = response['predicted_close']\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Save updated checkpoint\n",
    "    checkpoint['predictions'] = predictions\n",
    "    checkpoint['llm_results'] = llm_results\n",
    "    checkpoint['last_idx'] = len(predictions) - 1\n",
    "    checkpoint['completed'] = True\n",
    "    \n",
    "    with open(checkpoint_info['checkpoint_file'], 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Fixed {fixed_count}/{len(checkpoint_info['failed_indices'])} predictions\")\n",
    "    if still_failed:\n",
    "        print(f\"‚ö†Ô∏è Still failed: {len(still_failed)} predictions (using fallback values)\")\n",
    "        print(f\"   Indices: {still_failed[:10]}\")\n",
    "    print(f\"üíæ Updated checkpoint saved to: {checkpoint_info['checkpoint_file']}\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "# Fix training data\n",
    "\n",
    "\n",
    "# Fix test data\n",
    "if test_check and test_check['failed'] > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FIXING TEST DATA\")\n",
    "    print(\"=\"*80)\n",
    "    test_checkpoint_fixed = fix_failed_predictions(test_check, test_data, \"Test\")\n",
    "    # Update global variables\n",
    "    llm_predictions = test_checkpoint_fixed['predictions']\n",
    "    actual_prices = test_checkpoint_fixed['actual_prices']\n",
    "    llm_results = test_checkpoint_fixed['llm_results']\n",
    "    print(f\"‚úÖ Test data updated: {len(llm_predictions)} predictions\")\n",
    "    \n",
    "    # Update test_df\n",
    "    test_df['llm_prediction'] = llm_predictions\n",
    "    test_df['actual_price'] = actual_prices\n",
    "    \n",
    "    # Update justification features\n",
    "    justifications = []\n",
    "    likelihoods = []\n",
    "    feature_rows = []\n",
    "    for res in llm_results:\n",
    "        res = res if isinstance(res, dict) else {}\n",
    "        justification = res.get('justification', '')\n",
    "        justifications.append(justification)\n",
    "        likelihoods.append(safe_float(res.get('likelihood'), 0.5))\n",
    "        feature_rows.append(extract_justification_features(justification))\n",
    "    \n",
    "    test_df['llm_justification'] = justifications\n",
    "    test_df['llm_likelihood'] = likelihoods\n",
    "    \n",
    "    feature_keys = list(feature_rows[0].keys()) if feature_rows else []\n",
    "    for key in feature_keys:\n",
    "        test_df[key] = [row[key] for row in feature_rows]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL FAILED PREDICTIONS HAVE BEEN PROCESSED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"You can now proceed with PPO training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
