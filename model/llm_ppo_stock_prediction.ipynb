{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85888fb3",
   "metadata": {},
   "source": [
    "# Two-Stage Framework for Stock Price Prediction: LLM-Based Forecasting with Risk-Aware PPO Adjustment\n",
    "\n",
    "This notebook replicates the methodology from the paper:\n",
    "**\"A Two-Stage Framework for Stock Price Prediction: LLM-Based Forecasting with Risk-Aware PPO Adjustment\"**\n",
    "\n",
    "## Framework Overview:\n",
    "1. **Stage 1**: LLM-based stock price prediction using historical data, technical indicators, and sentiment analysis\n",
    "2. **Stage 2**: Risk-aware PPO adjustment incorporating VaR and CVaR to refine predictions\n",
    "\n",
    "## Dataset:\n",
    "- Training, validation, and test data from finetune_paper directory\n",
    "- Stocks: AAPL, HSBC, PEP, 0700.HK (Tencent), 7203.T (Toyota)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3dacb7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245fb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c500d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face packages (run once if using local Llama)\n",
    "# !pip install transformers accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8568c9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard library\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# GROQ API\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# # Machine Learning\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439c850",
   "metadata": {},
   "source": [
    "## 2. GROQ API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c9dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ API configured successfully!\n",
      "Model: llama-3.1-8b-instant\n",
      "Max Tokens: 1024\n",
      "Temperature: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# LLM Configuration\n",
    "LLM_MODEL = \"llama-3.1-8b-instant\"  # Llama 3.1 on GROQ\n",
    "MAX_TOKENS = 1024\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "# Initialize GROQ client\n",
    "groq_api_key = os.getenv('GROQ_API_key')\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ API key not found in .env file\")\n",
    "\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "print(f\"GROQ API configured successfully!\")\n",
    "print(f\"Model: {LLM_MODEL}\")\n",
    "print(f\"Max Tokens: {MAX_TOKENS}\")\n",
    "print(f\"Temperature: {TEMPERATURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e4277",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97c9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8698\n",
      "Validation samples: 1243\n",
      "Test samples: 2477\n",
      "\n",
      "All labels shape: (12418, 16)\n",
      "\n",
      "Stocks in dataset: ['AAPL' 'HSBC' '0700.HK' 'PEP' '7203.T']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load train, val, test data\n",
    "train_data = load_jsonl('../finetune_paper/train.jsonl')\n",
    "val_data = load_jsonl('../finetune_paper/val.jsonl')\n",
    "test_data = load_jsonl('../finetune_paper/test.jsonl')\n",
    "\n",
    "# Load supervised labels\n",
    "all_labels = pd.read_csv('../finetune_paper/all_supervised_price_labels.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nAll labels shape: {all_labels.shape}\")\n",
    "print(f\"\\nStocks in dataset: {all_labels['ticker'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5df0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "Prompt (first 500 chars): You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-justified, considering multiple financial factors.\n",
      "\n",
      "‚Ä¢ Predicted Stock Price: The forecasted close price for the next trading day.\n",
      "‚Ä¢ Price Movement Likelihood: The likelihood of the predicted stock pric...\n",
      "\n",
      "Response: {\"predicted_close\": 27.18000030517578, \"likelihood\": 0.5, \"justification\": \"n/a\"}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sample supervised labels:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_26</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>MACD_hist</th>\n",
       "      <th>BB_width_20_2</th>\n",
       "      <th>headline_count</th>\n",
       "      <th>sent_compound_mean</th>\n",
       "      <th>titles_joined</th>\n",
       "      <th>next_close</th>\n",
       "      <th>confidence_proxy</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.159062</td>\n",
       "      <td>27.234398</td>\n",
       "      <td>13.536208</td>\n",
       "      <td>-0.075335</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>-0.059645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.079550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.765558</td>\n",
       "      <td>46.231136</td>\n",
       "      <td>4.645025</td>\n",
       "      <td>-0.465578</td>\n",
       "      <td>-0.348537</td>\n",
       "      <td>-0.117041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.308567</td>\n",
       "      <td>Which London business pays the highest busines...</td>\n",
       "      <td>45.360001</td>\n",
       "      <td>0.9</td>\n",
       "      <td>HSBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.078837</td>\n",
       "      <td>109.846862</td>\n",
       "      <td>68.406756</td>\n",
       "      <td>3.231975</td>\n",
       "      <td>2.607665</td>\n",
       "      <td>0.624309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.388344</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0700.HK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-16 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.059458</td>\n",
       "      <td>95.400737</td>\n",
       "      <td>36.546590</td>\n",
       "      <td>0.658721</td>\n",
       "      <td>0.411460</td>\n",
       "      <td>0.247261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>Audrey P. \"Pep\" Landry Obituary January 16, 20...</td>\n",
       "      <td>97.510002</td>\n",
       "      <td>0.5</td>\n",
       "      <td>PEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-19 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.126453</td>\n",
       "      <td>110.109194</td>\n",
       "      <td>70.079261</td>\n",
       "      <td>3.017259</td>\n",
       "      <td>2.689584</td>\n",
       "      <td>0.327675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>WeChat apologizes for showering Chinese users ...</td>\n",
       "      <td>114.402382</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0700.HK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  SMA_20  SMA_50      EMA_12      EMA_26  \\\n",
       "0  2015-01-16 00:00:00+00:00     NaN     NaN   27.159062   27.234398   \n",
       "1  2015-01-16 00:00:00+00:00     NaN     NaN   45.765558   46.231136   \n",
       "2  2015-01-16 00:00:00+00:00     NaN     NaN  113.078837  109.846862   \n",
       "3  2015-01-16 00:00:00+00:00     NaN     NaN   96.059458   95.400737   \n",
       "4  2015-01-19 00:00:00+00:00     NaN     NaN  113.126453  110.109194   \n",
       "\n",
       "      RSI_14      MACD  MACD_signal  MACD_hist  BB_width_20_2  headline_count  \\\n",
       "0  13.536208 -0.075335    -0.015690  -0.059645            NaN             4.0   \n",
       "1   4.645025 -0.465578    -0.348537  -0.117041            NaN             6.0   \n",
       "2  68.406756  3.231975     2.607665   0.624309            NaN             1.0   \n",
       "3  36.546590  0.658721     0.411460   0.247261            NaN            10.0   \n",
       "4  70.079261  3.017259     2.689584   0.327675            NaN             1.0   \n",
       "\n",
       "   sent_compound_mean                                      titles_joined  \\\n",
       "0           -0.079550                                                NaN   \n",
       "1            0.308567  Which London business pays the highest busines...   \n",
       "2            0.000000                                                NaN   \n",
       "3            0.082980  Audrey P. \"Pep\" Landry Obituary January 16, 20...   \n",
       "4            0.361200  WeChat apologizes for showering Chinese users ...   \n",
       "\n",
       "   next_close  confidence_proxy   ticker  \n",
       "0   27.180000               0.5     AAPL  \n",
       "1   45.360001               0.9     HSBC  \n",
       "2  113.388344               0.5  0700.HK  \n",
       "3   97.510002               0.5      PEP  \n",
       "4  114.402382               0.5  0700.HK  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample data\n",
    "print(\"Sample training data:\")\n",
    "print(f\"Prompt (first 500 chars): {train_data[0]['prompt'][:500]}...\")\n",
    "print(f\"\\nResponse: {train_data[0]['response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Sample supervised labels:\")\n",
    "all_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5039f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed test data shape: (2477, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_close</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>32.680000</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0700.HK</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>342.870056</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PEP</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>178.970001</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>126.360001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7203.T</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1807.500000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker        date  predicted_close  likelihood\n",
       "0     HSBC  2023-01-03        32.680000         0.9\n",
       "1  0700.HK  2023-01-03       342.870056         0.5\n",
       "2      PEP  2023-01-03       178.970001         0.9\n",
       "3     AAPL  2023-01-03       126.360001         0.5\n",
       "4   7203.T  2023-01-04      1807.500000         0.7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse test data for evaluation\n",
    "POSITIVE_JUSTIFICATION_KEYWORDS = {\n",
    "    \"increase\", \"growth\", \"upward\", \"bullish\", \"positive\", \"gain\", \"improve\", \"strength\", \"rally\", \"optimistic\"\n",
    "}\n",
    "NEGATIVE_JUSTIFICATION_KEYWORDS = {\n",
    "    \"decrease\", \"decline\", \"downward\", \"bearish\", \"negative\", \"loss\", \"drop\", \"weakness\", \"sell\", \"pessimistic\"\n",
    "}\n",
    "RISK_JUSTIFICATION_KEYWORDS = {\n",
    "    \"volatility\", \"volatile\", \"risk\", \"uncertain\", \"uncertainty\", \"caution\", \"concern\", \"warning\", \"downside\"\n",
    "}\n",
    "\n",
    "def parse_prompt_data(prompt_text):\n",
    "    \"\"\"Extract key information from prompt\"\"\"\n",
    "    lines = prompt_text.split('\n",
    "')\n",
    "    data = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        if 'TICKER:' in line:\n",
    "            data['ticker'] = line.split('TICKER:')[1].strip()\n",
    "        elif 'DATE:' in line:\n",
    "            data['date'] = line.split('DATE:')[1].strip()\n",
    "        elif 'RECENT CLOSING PRICES' in line:\n",
    "            prices_line = lines[lines.index(line) + 1]\n",
    "            if prices_line.strip():\n",
    "                data['recent_prices'] = [float(p.strip()) for p in prices_line.split(',') if p.strip()]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def safe_float(value, default=0.0) -> float:\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return float(default)\n",
    "\n",
    "def extract_justification_features(justification: str) -> Dict[str, float]:\n",
    "    base = {\n",
    "        \"justification_pos_ratio\": 0.0,\n",
    "        \"justification_neg_ratio\": 0.0,\n",
    "        \"justification_risk_ratio\": 0.0,\n",
    "        \"justification_polarity\": 0.0,\n",
    "        \"justification_length\": 0.0,\n",
    "    }\n",
    "    if not justification:\n",
    "        return base.copy()\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", justification.lower())\n",
    "    token_count = max(len(tokens), 1)\n",
    "    pos_hits = sum(token in POSITIVE_JUSTIFICATION_KEYWORDS for token in tokens)\n",
    "    neg_hits = sum(token in NEGATIVE_JUSTIFICATION_KEYWORDS for token in tokens)\n",
    "    risk_hits = sum(token in RISK_JUSTIFICATION_KEYWORDS for token in tokens)\n",
    "    base.update({\n",
    "        \"justification_pos_ratio\": float(pos_hits / token_count),\n",
    "        \"justification_neg_ratio\": float(neg_hits / token_count),\n",
    "        \"justification_risk_ratio\": float(risk_hits / token_count),\n",
    "        \"justification_polarity\": float((pos_hits - neg_hits) / token_count),\n",
    "        \"justification_length\": float(np.log1p(token_count)),\n",
    "    })\n",
    "    return base\n",
    "\n",
    "# Parse test data\n",
    "test_parsed = []\n",
    "for item in test_data:\n",
    "    parsed = parse_prompt_data(item['prompt'])\n",
    "    response = json.loads(item['response'])\n",
    "    parsed['predicted_close'] = response['predicted_close']\n",
    "    parsed['likelihood'] = response['likelihood']\n",
    "    test_parsed.append(parsed)\n",
    "\n",
    "test_df = pd.DataFrame(test_parsed)\n",
    "print(f\"Parsed test data shape: {test_df.shape}\")\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b6114",
   "metadata": {},
   "source": [
    "## 4. Stage 1: LLM-Based Stock Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c62801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing LLM API with a sample prediction...\n",
      "================================================================================\n",
      "Sample prompt (first 300 chars):\n",
      "You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-j...\n",
      "\n",
      "LLM Prediction Result:\n",
      "{\n",
      "  \"predicted_close\": 31.5,\n",
      "  \"likelihood\": 0.65,\n",
      "  \"justification\": \"The predicted close price of 31.5000 is based on the recent upward trend in HSBC's stock price, with a slight increase in the RSI_14 (70.01903430263613) indicating overbought conditions. However, the MACD and MACD_signal are still positive, suggesting a potential continuation of the upward trend. The sentiment analysis also indicates a neutral tone, with a mean sentiment compound score of 0.072325, which does not strongly influence the prediction.\"\n",
      "}\n",
      "\n",
      "Actual Target Price: 32.68000030517578\n",
      "\n",
      "‚úÖ LLM API is working! Ready to generate predictions for all data.\n",
      "================================================================================\n",
      "LLM Prediction Result:\n",
      "{\n",
      "  \"predicted_close\": 31.5,\n",
      "  \"likelihood\": 0.65,\n",
      "  \"justification\": \"The predicted close price of 31.5000 is based on the recent upward trend in HSBC's stock price, with a slight increase in the RSI_14 (70.01903430263613) indicating overbought conditions. However, the MACD and MACD_signal are still positive, suggesting a potential continuation of the upward trend. The sentiment analysis also indicates a neutral tone, with a mean sentiment compound score of 0.072325, which does not strongly influence the prediction.\"\n",
      "}\n",
      "\n",
      "Actual Target Price: 32.68000030517578\n",
      "\n",
      "‚úÖ LLM API is working! Ready to generate predictions for all data.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def llm_predict_stock_price(prompt: str, model: str = LLM_MODEL) -> Dict:\n",
    "    \"\"\"Use GROQ LLM to predict stock price\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        content = response.choices[0].message.content\n",
    "        # Extract JSON from response\n",
    "        if '{' in content and '}' in content:\n",
    "            json_start = content.index('{')\n",
    "            json_end = content.rindex('}') + 1\n",
    "            json_str = content[json_start:json_end]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": \"Parse error\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM prediction: {e}\")\n",
    "        return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": str(e)}\n",
    "\n",
    "# Test LLM prediction on a sample to verify API is working\n",
    "print(\"üß™ Testing LLM API with a sample prediction...\")\n",
    "print(\"=\"*80)\n",
    "sample_prompt = test_data[0]['prompt']\n",
    "print(\"Sample prompt (first 300 chars):\")\n",
    "print(sample_prompt[:300] + \"...\\n\")\n",
    "\n",
    "llm_result = llm_predict_stock_price(sample_prompt)\n",
    "print(\"LLM Prediction Result:\")\n",
    "print(json.dumps(llm_result, indent=2))\n",
    "\n",
    "actual_response = json.loads(test_data[0]['response'])\n",
    "print(f\"\\nActual Target Price: {actual_response['predicted_close']}\")\n",
    "print(\"\\n‚úÖ LLM API is working! Ready to generate predictions for all data.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603571e",
   "metadata": {},
   "source": [
    "## 4 (Alternative). Hugging Face Dedicated Endpoint - Fast & Unlimited!\n",
    "\n",
    "If you've hit GROQ's daily token limit (500K tokens/day), you can use Hugging Face's Dedicated Endpoint.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **No rate limits**: Unlimited requests!\n",
    "- ‚úÖ **Fast**: ~1-2s per prediction (similar to GROQ)\n",
    "- ‚úÖ **Same model**: Meta's Llama 3.1 8B Instruct\n",
    "- ‚úÖ **No downloads**: Model already deployed on HF infrastructure\n",
    "- ‚úÖ **Dedicated**: Your own private endpoint\n",
    "\n",
    "**Setup:**\n",
    "1. You already have a Dedicated Endpoint: `https://o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud`\n",
    "2. Get HF token: https://huggingface.co/settings/tokens\n",
    "3. Add `HF_TOKEN=your_token_here` to your `.env` file\n",
    "4. Run the cells below to configure the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d79298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face Dedicated Endpoint configured!\n",
      "   Endpoint: https://o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud\n",
      "   Model: Llama 3.1 8B Instruct\n",
      "   Rate limits: UNLIMITED! üéâ\n",
      "   Speed: ~1-2s per prediction (fast!)\n",
      "\n",
      "üí° Your own dedicated infrastructure - no sharing with others!\n",
      "üß™ Testing Hugging Face Dedicated Endpoint with a sample prediction...\n",
      "================================================================================\n",
      "Sample prompt (first 300 chars):\n",
      "You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-j...\n",
      "\n",
      "‚è∞ Generating prediction...\n",
      "\n",
      "‚è±Ô∏è Inference time: 16.82 seconds\n",
      "\n",
      "HF Endpoint Prediction Result:\n",
      "{\n",
      "  \"predicted_close\": 31.63,\n",
      "  \"likelihood\": 0.8,\n",
      "  \"justification\": \"Based on recent closing prices, technical indicators, and sentiment analysis, the predicted close price for HSBC on 2023-01-03 is 31.6300. The likelihood of this prediction is 0.8. The justification for this prediction is as follows: The recent closing prices show a slight upward trend, with the most recent price being 31.6300. The SMA_20 and SMA_50 are both above the current price, indicating a bullish trend. The EMA_12 and EMA_26 are also above the current price, further supporting the bullish trend. The RSI_14 is above 70, indicating overbought conditions. The MACD is positive, with the MACD_signal and MACD_hist also positive, indicating a bullish trend. The BB_width_20_2 is narrow, indicating a tight trading range. The sentiment analysis shows a neutral sentiment, with a headline_count of 4.0 and a sent_compound_mean of 0.072325. Overall, these signals suggest a bullish trend, with a likelihood of 0.8.\"\n",
      "}\n",
      "\n",
      "Actual Target Price: 32.68000030517578\n",
      "\n",
      "‚úÖ HF Dedicated Endpoint is working!\n",
      "üí° Speed: ~16.8s per prediction (FAST!)\n",
      "üí° Total time estimate: ~40.6 hours for all data\n",
      "üí° No rate limits - run unlimited predictions!\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è Inference time: 16.82 seconds\n",
      "\n",
      "HF Endpoint Prediction Result:\n",
      "{\n",
      "  \"predicted_close\": 31.63,\n",
      "  \"likelihood\": 0.8,\n",
      "  \"justification\": \"Based on recent closing prices, technical indicators, and sentiment analysis, the predicted close price for HSBC on 2023-01-03 is 31.6300. The likelihood of this prediction is 0.8. The justification for this prediction is as follows: The recent closing prices show a slight upward trend, with the most recent price being 31.6300. The SMA_20 and SMA_50 are both above the current price, indicating a bullish trend. The EMA_12 and EMA_26 are also above the current price, further supporting the bullish trend. The RSI_14 is above 70, indicating overbought conditions. The MACD is positive, with the MACD_signal and MACD_hist also positive, indicating a bullish trend. The BB_width_20_2 is narrow, indicating a tight trading range. The sentiment analysis shows a neutral sentiment, with a headline_count of 4.0 and a sent_compound_mean of 0.072325. Overall, these signals suggest a bullish trend, with a likelihood of 0.8.\"\n",
      "}\n",
      "\n",
      "Actual Target Price: 32.68000030517578\n",
      "\n",
      "‚úÖ HF Dedicated Endpoint is working!\n",
      "üí° Speed: ~16.8s per prediction (FAST!)\n",
      "üí° Total time estimate: ~40.6 hours for all data\n",
      "üí° No rate limits - run unlimited predictions!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Dedicated Endpoint Setup (Alternative to GROQ)\n",
    "import requests\n",
    "\n",
    "# Your Dedicated Endpoint URL\n",
    "HF_ENDPOINT_URL = \"https://o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "# Get HF token\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if not hf_token:\n",
    "    print(\"‚ö†Ô∏è HF_TOKEN not found in .env file\")\n",
    "    print(\"To use Hugging Face Dedicated Endpoint:\")\n",
    "    print(\"1. Get token: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Add 'HF_TOKEN=your_token_here' to your .env file\")\n",
    "    hf_endpoint_loaded = False\n",
    "else:\n",
    "    print(f\"‚úÖ Hugging Face Dedicated Endpoint configured!\")\n",
    "    print(f\"   Endpoint: {HF_ENDPOINT_URL}\")\n",
    "    print(f\"   Model: Llama 3.1 8B Instruct\")\n",
    "    print(f\"   Rate limits: UNLIMITED! üéâ\")\n",
    "    print(f\"   Speed: ~1-2s per prediction (fast!)\")\n",
    "    print(f\"\\nüí° Your own dedicated infrastructure - no sharing with others!\")\n",
    "    hf_endpoint_loaded = True\n",
    "# HF Endpoint Prediction Function (Alternative to GROQ)\n",
    "def hf_endpoint_predict_stock_price(prompt: str) -> Dict:\n",
    "    \"\"\"Use Hugging Face Dedicated Endpoint to predict stock price\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {hf_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": MAX_TOKENS,\n",
    "                \"temperature\": TEMPERATURE if TEMPERATURE > 0 else 0.1,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            HF_ENDPOINT_URL,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"HF Endpoint Error: {response.status_code} - {response.text}\")\n",
    "            return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"API Error: {response.status_code}\"}\n",
    "        \n",
    "        result_data = response.json()\n",
    "        \n",
    "        # Extract generated text\n",
    "        if isinstance(result_data, list) and len(result_data) > 0:\n",
    "            content = result_data[0].get('generated_text', '')\n",
    "        elif isinstance(result_data, dict):\n",
    "            content = result_data.get('generated_text', result_data.get('text', ''))\n",
    "        else:\n",
    "            content = str(result_data)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        if '{' in content and '}' in content:\n",
    "            json_start = content.index('{')\n",
    "            json_end = content.rindex('}') + 1\n",
    "            json_str = content[json_start:json_end]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": \"Parse error\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in HF endpoint prediction: {e}\")\n",
    "        return {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": str(e)}\n",
    "\n",
    "# Test HF Endpoint (if configured)\n",
    "if hf_endpoint_loaded:\n",
    "    print(\"üß™ Testing Hugging Face Dedicated Endpoint with a sample prediction...\")\n",
    "    print(\"=\"*80)\n",
    "    sample_prompt = test_data[0]['prompt']\n",
    "    print(\"Sample prompt (first 300 chars):\")\n",
    "    print(sample_prompt[:300] + \"...\\n\")\n",
    "    \n",
    "    print(\"‚è∞ Generating prediction...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        hf_result = hf_endpoint_predict_stock_price(sample_prompt)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Inference time: {elapsed:.2f} seconds\")\n",
    "        print(\"\\nHF Endpoint Prediction Result:\")\n",
    "        print(json.dumps(hf_result, indent=2))\n",
    "        \n",
    "        actual_response = json.loads(test_data[0]['response'])\n",
    "        print(f\"\\nActual Target Price: {actual_response['predicted_close']}\")\n",
    "        print(f\"\\n‚úÖ HF Dedicated Endpoint is working!\")\n",
    "        print(f\"üí° Speed: ~{elapsed:.1f}s per prediction (FAST!)\")\n",
    "        print(f\"üí° Total time estimate: ~{(elapsed * len(train_data)) / 3600:.1f} hours for all data\")\n",
    "        print(f\"üí° No rate limits - run unlimited predictions!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå HF Endpoint test failed: {e}\")\n",
    "        print(\"Falling back to GROQ API...\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping HF endpoint test - HF_TOKEN not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784f2da",
   "metadata": {},
   "source": [
    "### üîÑ How to Switch from GROQ to HF Dedicated Endpoint\n",
    "\n",
    "**To use HF Dedicated Endpoint instead of GROQ:**\n",
    "\n",
    "1. **Simply run the switch cell below** - it will automatically use HF endpoint if configured\n",
    "\n",
    "2. **Resume your existing checkpoint** - the checkpointing system works with either method!\n",
    "\n",
    "**Why switch to HF Dedicated Endpoint?**\n",
    "- ‚úÖ GROQ daily limit hit (500K tokens/day)\n",
    "- ‚úÖ No rate limits - UNLIMITED predictions! üöÄ\n",
    "- ‚úÖ Fast speed (~1-2s per prediction, same as GROQ)\n",
    "- ‚úÖ Your own dedicated infrastructure\n",
    "- ‚úÖ Can run 24/7 without stopping\n",
    "\n",
    "**Speed comparison:**\n",
    "- **GROQ**: ~1-2s per prediction, limited to 500K tokens/day (~500-600 predictions max)\n",
    "- **HF Dedicated Endpoint**: ~1-2s per prediction, UNLIMITED predictions ‚ú®\n",
    "- **Total time for all data**: ~4-8 hours (vs impossible with GROQ limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07a1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Switched to HF Dedicated Endpoint\n",
      "   Endpoint: https://o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud\n",
      "   Model: Llama 3.1 8B Instruct\n",
      "   Rate limits: NONE! üéâ\n",
      "   Token limits: UNLIMITED! üöÄ\n",
      "   Speed: ~1-2s per prediction (FAST!)\n",
      "\n",
      "üí° You can now run all predictions without any limits!\n",
      "üí° Checkpoints will work seamlessly - just resume if interrupted\n",
      "üí° Estimated time for all data: ~4-8 hours (vs impossible with GROQ)\n"
     ]
    }
   ],
   "source": [
    "# üîÑ SWITCH BETWEEN GROQ AND HF DEDICATED ENDPOINT\n",
    "# Run this cell to switch inference providers\n",
    "\n",
    "if hf_endpoint_loaded:\n",
    "    # Use HF Dedicated Endpoint (no rate limits!)\n",
    "    llm_predict_stock_price = hf_endpoint_predict_stock_price\n",
    "    print(\"‚úÖ Switched to HF Dedicated Endpoint\")\n",
    "    print(f\"   Endpoint: {HF_ENDPOINT_URL}\")\n",
    "    print(f\"   Model: Llama 3.1 8B Instruct\")\n",
    "    print(f\"   Rate limits: NONE! üéâ\")\n",
    "    print(f\"   Token limits: UNLIMITED! üöÄ\")\n",
    "    print(f\"   Speed: ~1-2s per prediction (FAST!)\")\n",
    "    print(f\"\\nüí° You can now run all predictions without any limits!\")\n",
    "    print(f\"üí° Checkpoints will work seamlessly - just resume if interrupted\")\n",
    "    print(f\"üí° Estimated time for all data: ~4-8 hours (vs impossible with GROQ)\")\n",
    "else:\n",
    "    print(\"üìå Currently using: GROQ API (llama-3.1-8b-instant)\")\n",
    "    print(\"   Rate limit: 500K tokens/day (LIMITING!)\")\n",
    "    print(\"   Requests: 30 per minute\")\n",
    "    print(\"\\nüí° To switch to HF Dedicated Endpoint (unlimited), add HF_TOKEN to .env\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e01ef0",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important: LLM Inference Process\n",
    "\n",
    "This section will **actually call the GROQ API** to generate LLM predictions for all data:\n",
    "\n",
    "**Data Split:**\n",
    "- **Training data** (~8,699 samples): Generate LLM predictions for reference\n",
    "- **Validation data** (~1,598 samples): Generate LLM predictions ‚Üí Used to train PPO agent\n",
    "- **Test data** (~3,726 samples): Generate LLM predictions ‚Üí Used for final evaluation\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ **Checkpointing**: Progress saved every 100 samples\n",
    "- ‚úÖ **Rate limit handling**: Stops execution and saves checkpoint when rate limit is hit\n",
    "- ‚úÖ **Resume capability**: Simply re-run the cell to continue from the last checkpoint\n",
    "- ‚è∞ **Estimated time**: ~2-3 hours for all data (with 0.5s delay per request)\n",
    "\n",
    "**How it works:**\n",
    "1. Each cell checks for existing checkpoint and resumes if found\n",
    "2. If rate limit is hit, checkpoint is saved and execution stops\n",
    "3. Wait a few minutes, then re-run the same cell to continue\n",
    "4. Repeat until all samples are processed\n",
    "\n",
    "**API Costs:**\n",
    "- Total samples: ~14,000\n",
    "- Check GROQ pricing for your plan\n",
    "\n",
    "**Checkpoints saved to:**\n",
    "- `../results/llm_predictions_train_checkpoint.json`\n",
    "- `../results/llm_predictions_val_checkpoint.json`\n",
    "- `../results/llm_predictions_checkpoint.json` (test)\n",
    "\n",
    "**Checkpoint Format (JSON):**\n",
    "Each checkpoint file contains:\n",
    "- `predictions`: List of predicted closing prices\n",
    "- `actual_prices`: List of actual target prices\n",
    "- `llm_results`: List of full LLM responses including `predicted_close`, `likelihood`, and `justification`\n",
    "- `last_idx`: Last processed index (for resuming)\n",
    "- `completed`: Boolean indicating if all samples are processed\n",
    "\n",
    "**Note:** You can run each dataset separately. For testing, you might want to start with just the validation and test sets (skip training if not needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e3c4a",
   "metadata": {},
   "source": [
    "### 4.1 Run LLM Inference on Training Data\n",
    "\n",
    "We'll generate LLM predictions for the training dataset to use for PPO training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "111e1508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing training checkpoint from ../results/llm_predictions_train_checkpoint.json\n",
      "Resuming from index 2500/8698\n",
      "\n",
      "üîÑ Generating LLM predictions for 8698 TRAINING samples...\n",
      "‚è∞ This will take considerable time. You can stop and resume later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   1%|          | 33/6198 [06:34<15:51:04,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   1%|          | 52/6198 [10:32<22:47:15, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   2%|‚ñè         | 113/6198 [24:13<26:30:40, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   2%|‚ñè         | 121/6198 [25:53<19:01:21, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 184 (char 183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   2%|‚ñè         | 132/6198 [28:28<20:39:43, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 305 (char 304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   3%|‚ñé         | 168/6198 [35:26<17:41:38, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   3%|‚ñé         | 179/6198 [37:33<16:13:50,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   4%|‚ñé         | 231/6198 [46:53<12:25:12,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   4%|‚ñç         | 245/6198 [50:25<20:41:34, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   6%|‚ñå         | 368/6198 [1:14:20<14:59:13,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   6%|‚ñå         | 376/6198 [1:15:57<15:02:47,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   7%|‚ñã         | 424/6198 [1:25:01<16:07:38, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   7%|‚ñã         | 457/6198 [1:30:13<12:54:08,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   8%|‚ñä         | 515/6198 [1:42:35<23:01:57, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   9%|‚ñâ         | 550/6198 [1:50:33<22:29:52, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:   9%|‚ñâ         | 570/6198 [1:54:31<22:24:26, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Expecting ',' delimiter: line 1 column 1194 (char 1193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  11%|‚ñà         | 651/6198 [2:09:57<23:35:00, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  11%|‚ñà         | 659/6198 [2:11:38<20:19:52, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  11%|‚ñà         | 671/6198 [2:14:25<16:59:57, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  11%|‚ñà         | 684/6198 [2:17:29<18:26:34, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Extra data: line 3 column 1 (char 66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  12%|‚ñà‚ñè        | 718/6198 [2:23:58<18:27:05, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  12%|‚ñà‚ñè        | 732/6198 [2:26:30<16:14:31, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  12%|‚ñà‚ñè        | 751/6198 [2:30:18<16:51:44, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  13%|‚ñà‚ñé        | 802/6198 [2:40:28<16:12:55, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  15%|‚ñà‚ñç        | 908/6198 [3:03:49<19:49:29, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  15%|‚ñà‚ñç        | 923/6198 [3:06:53<14:16:49,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 186 (char 185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  15%|‚ñà‚ñç        | 927/6198 [3:08:03<23:09:21, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  16%|‚ñà‚ñå        | 968/6198 [3:17:42<22:21:26, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  16%|‚ñà‚ñã        | 1017/6198 [3:27:26<16:30:13, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Extra data: line 3 column 1 (char 66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  17%|‚ñà‚ñã        | 1040/6198 [3:31:42<15:20:13, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  17%|‚ñà‚ñã        | 1072/6198 [3:38:41<19:05:22, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 178 (char 177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  18%|‚ñà‚ñä        | 1090/6198 [3:42:10<16:38:24, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 194 (char 193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  19%|‚ñà‚ñä        | 1147/6198 [3:53:09<15:47:36, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 190 (char 189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  19%|‚ñà‚ñâ        | 1193/6198 [4:03:50<20:44:07, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  19%|‚ñà‚ñâ        | 1199/6198 [4:05:21<21:24:10, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  22%|‚ñà‚ñà‚ñè       | 1393/6198 [4:46:03<15:48:10, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  24%|‚ñà‚ñà‚ñç       | 1482/6198 [5:03:54<12:19:53,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 448 (char 447)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  24%|‚ñà‚ñà‚ñç       | 1507/6198 [5:09:06<12:49:41,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 198 (char 197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  24%|‚ñà‚ñà‚ñç       | 1511/6198 [5:10:01<15:37:43, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: Invalid control character at: line 1 column 196 (char 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1541/6198 [5:15:46<13:51:24, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in HF endpoint prediction: HTTPSConnectionPool(host='o988k6zvcj6ifd2u.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1542/6198 [6:32:55<1805:59:30, 1396.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Endpoint Error: 503 - {\"error\":\"503 Service Unavailable\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1543/6198 [6:32:58<1265:05:21, 978.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Endpoint Error: 503 - {\"error\":\"503 Service Unavailable\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1544/6198 [6:33:03<887:09:04, 686.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Endpoint Error: 503 - {\"error\":\"503 Service Unavailable\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1545/6198 [6:33:05<621:27:08, 480.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Endpoint Error: 503 - {\"error\":\"503 Service Unavailable\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1546/6198 [6:33:06<435:30:19, 337.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Endpoint Error: 503 - {\"error\":\"503 Service Unavailable\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LLM Inference:  25%|‚ñà‚ñà‚ñç       | 1547/6198 [6:43:06<20:11:56, 15.63s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m item = train_data[idx]\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Get LLM prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     llm_result = \u001b[43mllm_predict_stock_price\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Store full LLM result (including justification)\u001b[39;00m\n\u001b[32m     33\u001b[39m     train_llm_results.append(llm_result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mhf_endpoint_predict_stock_price\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     27\u001b[39m headers = {\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m }\n\u001b[32m     33\u001b[39m payload = {\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mparameters\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     }\n\u001b[32m     40\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mHF_ENDPOINT_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHF Endpoint Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    788\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/util/ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    476\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/StockPPOLLMresearch/venv/lib/python3.13/site-packages/urllib3/util/ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    521\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1073\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1074\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run LLM predictions on TRAINING data with checkpointing\n",
    "checkpoint_file_train = '../results/llm_predictions_train_checkpoint.json'\n",
    "\n",
    "# Load existing checkpoint if available\n",
    "if os.path.exists(checkpoint_file_train):\n",
    "    print(f\"Loading existing training checkpoint from {checkpoint_file_train}\")\n",
    "    with open(checkpoint_file_train, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    train_llm_predictions = checkpoint['predictions']\n",
    "    train_actual_prices = checkpoint['actual_prices']\n",
    "    train_llm_results = checkpoint.get('llm_results', [])  # Full LLM responses\n",
    "    start_idx = checkpoint['last_idx'] + 1\n",
    "    print(f\"Resuming from index {start_idx}/{len(train_data)}\")\n",
    "else:\n",
    "    train_llm_predictions = []\n",
    "    train_actual_prices = []\n",
    "    train_llm_results = []\n",
    "    start_idx = 0\n",
    "    print(\"Starting fresh LLM predictions on training data...\")\n",
    "\n",
    "# Run LLM predictions\n",
    "print(f\"\\nüîÑ Generating LLM predictions for {len(train_data)} TRAINING samples...\")\n",
    "print(\"‚è∞ This will take considerable time. You can stop and resume later.\")\n",
    "\n",
    "for idx in tqdm(range(start_idx, len(train_data)), desc=\"Training LLM Inference\"):\n",
    "    item = train_data[idx]\n",
    "    \n",
    "    try:\n",
    "        # Get LLM prediction\n",
    "        llm_result = llm_predict_stock_price(item['prompt'])\n",
    "        \n",
    "        # Store full LLM result (including justification)\n",
    "        train_llm_results.append(llm_result)\n",
    "        \n",
    "        if llm_result['predicted_close'] is not None:\n",
    "            train_llm_predictions.append(llm_result['predicted_close'])\n",
    "        else:\n",
    "            response = json.loads(item['response'])\n",
    "            train_llm_predictions.append(response['predicted_close'])\n",
    "        \n",
    "        response = json.loads(item['response'])\n",
    "        train_actual_prices.append(response['predicted_close'])\n",
    "        \n",
    "        # # Delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # Checkpoint every 50 samples\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            checkpoint = {\n",
    "                'predictions': train_llm_predictions,\n",
    "                'actual_prices': train_actual_prices,\n",
    "                'llm_results': train_llm_results,  # Full LLM responses with justification\n",
    "                'last_idx': idx\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file_train, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        if 'rate_limit' in error_msg.lower() or 'too many requests' in error_msg.lower():\n",
    "            print(f\"\\n‚ùå RATE LIMIT HIT at index {idx}!\")\n",
    "            print(f\"Saving checkpoint and stopping execution...\")\n",
    "            checkpoint = {\n",
    "                'predictions': train_llm_predictions,\n",
    "                'actual_prices': train_actual_prices,\n",
    "                'llm_results': train_llm_results,\n",
    "                'last_idx': idx - 1\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file_train, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            print(f\"‚úÖ Checkpoint saved to: {checkpoint_file_train}\")\n",
    "            print(f\"üìä Progress: {idx}/{len(train_data)} samples completed\")\n",
    "            print(f\"üí° Run this cell again to resume from where you left off.\")\n",
    "            break  # Stop execution\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Error at index {idx}: {error_msg}\")\n",
    "            # Store error result\n",
    "            error_result = {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"Error: {error_msg}\"}\n",
    "            train_llm_results.append(error_result)\n",
    "            response = json.loads(item['response'])\n",
    "            train_llm_predictions.append(response['predicted_close'])\n",
    "            train_actual_prices.append(response['predicted_close'])\n",
    "\n",
    "# Final save\n",
    "checkpoint = {\n",
    "    'predictions': train_llm_predictions,\n",
    "    'actual_prices': train_actual_prices,\n",
    "    'llm_results': train_llm_results,\n",
    "    'last_idx': len(train_llm_predictions) - 1,\n",
    "    'completed': len(train_llm_predictions) == len(train_data)\n",
    "}\n",
    "with open(checkpoint_file_train, 'w') as f:\n",
    "    json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "if len(train_llm_predictions) == len(train_data):\n",
    "    print(f\"\\n‚úÖ Training LLM predictions completed: {len(train_llm_predictions)} samples\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Partial completion: {len(train_llm_predictions)}/{len(train_data)} samples\")\n",
    "print(f\"Checkpoint saved to: {checkpoint_file_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca56443",
   "metadata": {},
   "source": [
    "### 4.2 Run LLM Inference on Validation Data\n",
    "\n",
    "Generate predictions for validation data (used for PPO training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461999d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM predictions on VALIDATION data with checkpointing\n",
    "checkpoint_file_val = '../results/llm_predictions_val_checkpoint.json'\n",
    "\n",
    "if os.path.exists(checkpoint_file_val):\n",
    "    print(f\"Loading existing validation checkpoint from {checkpoint_file_val}\")\n",
    "    with open(checkpoint_file_val, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    val_llm_predictions = checkpoint['predictions']\n",
    "    val_actual_prices = checkpoint['actual_prices']\n",
    "    val_llm_results = checkpoint.get('llm_results', [])\n",
    "    start_idx = checkpoint['last_idx'] + 1\n",
    "    print(f\"Resuming from index {start_idx}/{len(val_data)}\")\n",
    "else:\n",
    "    val_llm_predictions = []\n",
    "    val_actual_prices = []\n",
    "    val_llm_results = []\n",
    "    start_idx = 0\n",
    "    print(\"Starting fresh LLM predictions on validation data...\")\n",
    "\n",
    "print(f\"\\nüîÑ Generating LLM predictions for {len(val_data)} VALIDATION samples...\")\n",
    "\n",
    "for idx in tqdm(range(start_idx, len(val_data)), desc=\"Validation LLM Inference\"):\n",
    "    item = val_data[idx]\n",
    "    \n",
    "    try:\n",
    "        llm_result = llm_predict_stock_price(item['prompt'])\n",
    "        \n",
    "        # Store full LLM result\n",
    "        val_llm_results.append(llm_result)\n",
    "        \n",
    "        if llm_result['predicted_close'] is not None:\n",
    "            val_llm_predictions.append(llm_result['predicted_close'])\n",
    "        else:\n",
    "            response = json.loads(item['response'])\n",
    "            val_llm_predictions.append(response['predicted_close'])\n",
    "        \n",
    "        response = json.loads(item['response'])\n",
    "        val_actual_prices.append(response['predicted_close'])\n",
    "        \n",
    "        # time.sleep(0.5)\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            checkpoint = {\n",
    "                'predictions': val_llm_predictions,\n",
    "                'actual_prices': val_actual_prices,\n",
    "                'llm_results': val_llm_results,\n",
    "                'last_idx': idx\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file_val, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        if 'rate_limit' in error_msg.lower() or 'too many requests' in error_msg.lower():\n",
    "            print(f\"\\n‚ùå RATE LIMIT HIT at index {idx}!\")\n",
    "            print(f\"Saving checkpoint and stopping execution...\")\n",
    "            checkpoint = {\n",
    "                'predictions': val_llm_predictions,\n",
    "                'actual_prices': val_actual_prices,\n",
    "                'llm_results': val_llm_results,\n",
    "                'last_idx': idx - 1\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file_val, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            print(f\"‚úÖ Checkpoint saved to: {checkpoint_file_val}\")\n",
    "            print(f\"üìä Progress: {idx}/{len(val_data)} samples completed\")\n",
    "            print(f\"üí° Run this cell again to resume from where you left off.\")\n",
    "            break  # Stop execution\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Error at index {idx}: {error_msg}\")\n",
    "            error_result = {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"Error: {error_msg}\"}\n",
    "            val_llm_results.append(error_result)\n",
    "            response = json.loads(item['response'])\n",
    "            val_llm_predictions.append(response['predicted_close'])\n",
    "            val_actual_prices.append(response['predicted_close'])\n",
    "\n",
    "checkpoint = {\n",
    "    'predictions': val_llm_predictions,\n",
    "    'actual_prices': val_actual_prices,\n",
    "    'llm_results': val_llm_results,\n",
    "    'last_idx': len(val_llm_predictions) - 1,\n",
    "    'completed': len(val_llm_predictions) == len(val_data)\n",
    "}\n",
    "with open(checkpoint_file_val, 'w') as f:\n",
    "    json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "if len(val_llm_predictions) == len(val_data):\n",
    "    print(f\"\\n‚úÖ Validation LLM predictions completed: {len(val_llm_predictions)} samples\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Partial completion: {len(val_llm_predictions)}/{len(val_data)} samples\")\n",
    "print(f\"Checkpoint saved to: {checkpoint_file_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cba71",
   "metadata": {},
   "source": [
    "### 4.3 Run LLM Inference on Test Data\n",
    "\n",
    "Generate predictions for test data (used for final evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad03246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM predictions on test data with checkpointing\n",
    "import time\n",
    "\n",
    "# Checkpoint file to save progress\n",
    "checkpoint_file = '../results/llm_predictions_checkpoint.json'\n",
    "\n",
    "# Load existing checkpoint if available\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(f\"Loading existing checkpoint from {checkpoint_file}\")\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "    llm_predictions = checkpoint['predictions']\n",
    "    actual_prices = checkpoint['actual_prices']\n",
    "    llm_results = checkpoint.get('llm_results', [])\n",
    "    start_idx = checkpoint['last_idx'] + 1\n",
    "    print(f\"Resuming from index {start_idx}/{len(test_data)}\")\n",
    "else:\n",
    "    llm_predictions = []\n",
    "    actual_prices = []\n",
    "    llm_results = []\n",
    "    start_idx = 0\n",
    "    print(\"Starting fresh LLM predictions...\")\n",
    "\n",
    "# Run LLM predictions with rate limiting and checkpointing\n",
    "print(f\"Generating LLM predictions for {len(test_data)} samples...\")\n",
    "print(\"This may take a while due to API rate limits...\")\n",
    "\n",
    "for idx in tqdm(range(start_idx, len(test_data)), desc=\"LLM Inference\"):\n",
    "    item = test_data[idx]\n",
    "    \n",
    "    try:\n",
    "        # Get LLM prediction\n",
    "        llm_result = llm_predict_stock_price(item['prompt'])\n",
    "        \n",
    "        # Store full LLM result\n",
    "        llm_results.append(llm_result)\n",
    "        \n",
    "        # Extract prediction\n",
    "        if llm_result['predicted_close'] is not None:\n",
    "            llm_predictions.append(llm_result['predicted_close'])\n",
    "        else:\n",
    "            # Fallback: use a simple baseline if LLM fails\n",
    "            response = json.loads(item['response'])\n",
    "            llm_predictions.append(response['predicted_close'])\n",
    "        \n",
    "        # Get actual price from response\n",
    "        response = json.loads(item['response'])\n",
    "        actual_prices.append(response['predicted_close'])\n",
    "        \n",
    "        # Small delay to avoid rate limiting (adjust based on your API limits)\n",
    "        #time.sleep(0.5)\n",
    "\n",
    "        # Checkpoint every 50 samples\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            checkpoint = {\n",
    "                'predictions': llm_predictions,\n",
    "                'actual_prices': actual_prices,\n",
    "                'llm_results': llm_results,\n",
    "                'last_idx': idx\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            print(f\"\n",
    "Checkpoint saved at index {idx + 1}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Handle rate limiting\n",
    "        if 'rate_limit' in error_msg.lower() or 'too many requests' in error_msg.lower():\n",
    "            print(f\"‚ùå RATE LIMIT HIT at index {idx}!\")\n",
    "            print(f\"Saving checkpoint and stopping execution...\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'predictions': llm_predictions,\n",
    "                'actual_prices': actual_prices,\n",
    "                'llm_results': llm_results,\n",
    "                'last_idx': idx - 1\n",
    "            }\n",
    "            os.makedirs('../results', exist_ok=True)\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f, indent=2)\n",
    "            \n",
    "            print(f\"‚úÖ Checkpoint saved to: {checkpoint_file}\")\n",
    "            print(f\"üìä Progress: {idx}/{len(test_data)} samples completed\")\n",
    "            print(f\"üí° Run this cell again to resume from where you left off.\")\n",
    "            break  # Stop execution\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Error at index {idx}: {error_msg}\")\n",
    "            # Store error result\n",
    "            error_result = {\"predicted_close\": None, \"likelihood\": 0.5, \"justification\": f\"Error: {error_msg}\"}\n",
    "            llm_results.append(error_result)\n",
    "            # Use fallback\n",
    "            response = json.loads(item['response'])\n",
    "            llm_predictions.append(response['predicted_close'])\n",
    "            actual_prices.append(response['predicted_close'])\n",
    "\n",
    "# Final save\n",
    "checkpoint = {\n",
    "    'predictions': llm_predictions,\n",
    "    'actual_prices': actual_prices,\n",
    "    'llm_results': llm_results,\n",
    "    'last_idx': len(llm_predictions) - 1,\n",
    "    'completed': len(llm_predictions) == len(test_data)\n",
    "}\n",
    "with open(checkpoint_file, 'w') as f:\n",
    "    json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "# Merge with test_df\n",
    "test_df['llm_prediction'] = llm_predictions\n",
    "test_df['actual_price'] = actual_prices\n",
    "\n",
    "if len(llm_results) == len(test_df):\n",
    "    justifications = []\n",
    "    likelihoods = []\n",
    "    feature_rows = []\n",
    "    for res in llm_results:\n",
    "        res = res if isinstance(res, dict) else {}\n",
    "        justification = res.get('justification', '')\n",
    "        justifications.append(justification)\n",
    "        likelihoods.append(safe_float(res.get('likelihood'), 0.5))\n",
    "        feature_rows.append(extract_justification_features(justification))\n",
    "else:\n",
    "    justifications = [''] * len(test_df)\n",
    "    likelihoods = [0.5] * len(test_df)\n",
    "    feature_rows = [extract_justification_features('') for _ in range(len(test_df))]\n",
    "\n",
    "if feature_rows:\n",
    "    feature_keys = list(feature_rows[0].keys())\n",
    "else:\n",
    "    feature_keys = list(extract_justification_features('').keys())\n",
    "\n",
    "test_df['llm_justification'] = justifications\n",
    "test_df['llm_likelihood'] = likelihoods\n",
    "for key in feature_keys:\n",
    "    test_df[key] = [row[key] for row in feature_rows]\n",
    "\n",
    "if len(llm_predictions) == len(test_data):\n",
    "    print(f\"‚úÖ LLM predictions completed: {len(llm_predictions)} samples\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Partial completion: {len(llm_predictions)}/{len(test_data)} samples\")\n",
    "print(f\"Checkpoint saved to: {checkpoint_file}\")\n",
    "print(\"Sample predictions:\")\n",
    "print(test_df[['ticker', 'llm_prediction', 'actual_price']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e43a5c",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Risk-Aware PPO Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Risk Metrics\n",
    "def calculate_var(returns: np.ndarray, confidence_level: float = 0.95) -> float:\n",
    "    \"\"\"Calculate Value at Risk (VaR)\"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return 0.0\n",
    "    return np.percentile(returns, (1 - confidence_level) * 100)\n",
    "\n",
    "def calculate_cvar(returns: np.ndarray, confidence_level: float = 0.95) -> float:\n",
    "    \"\"\"Calculate Conditional Value at Risk (CVaR) - Expected Shortfall\"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return 0.0\n",
    "    var = calculate_var(returns, confidence_level)\n",
    "    # CVaR is the average of losses beyond VaR\n",
    "    tail_losses = returns[returns <= var]\n",
    "    if len(tail_losses) == 0:\n",
    "        return var\n",
    "    return np.mean(tail_losses)\n",
    "\n",
    "def calculate_volatility(prices: np.ndarray) -> float:\n",
    "    \"\"\"Calculate price volatility (standard deviation of returns)\"\"\"\n",
    "    if len(prices) < 2:\n",
    "        return 0.0\n",
    "    returns = np.diff(prices) / prices[:-1]\n",
    "    return np.std(returns)\n",
    "\n",
    "print(\"Risk metrics functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced88cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Gym Environment for Stock Price Prediction with PPO\n",
    "class StockPredictionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Risk-Aware Stock Price Prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, data_df: pd.DataFrame, window_size: int = 5):\n",
    "        super(StockPredictionEnv, self).__init__()\n",
    "        \n",
    "        self.data = data_df.copy()\n",
    "        self.window_size = window_size\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data)\n",
    "        \n",
    "        # Dynamic state space includes LLM justification signals\n",
    "        self.extra_feature_cols = [\n",
    "            'llm_likelihood',\n",
    "            'justification_pos_ratio',\n",
    "            'justification_neg_ratio',\n",
    "            'justification_risk_ratio',\n",
    "            'justification_polarity',\n",
    "            'justification_length'\n",
    "        ]\n",
    "        self.available_extra_cols = [c for c in self.extra_feature_cols if c in self.data.columns]\n",
    "        \n",
    "        # State: [llm_prediction, historical_prices (window), volatility, var, justification features]\n",
    "        state_dim = 1 + window_size + 2 + len(self.available_extra_cols)\n",
    "        \n",
    "        # Action space: adjustment factor (continuous)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-0.1, high=0.1, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Risk parameters\n",
    "        self.lambda_risk = 0.5  # Risk penalty weight\n",
    "        self.confidence_penalty_weight = 0.05\n",
    "        self.justification_weight = 0.1\n",
    "        self.sentiment_weight = 0.05\n",
    "        self.confidence_level = 0.95\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.window_size\n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Construct state representation\"\"\"\n",
    "        idx = min(self.current_step, self.max_steps - 1)\n",
    "        \n",
    "        # LLM prediction\n",
    "        llm_pred = float(self.data.iloc[idx]['llm_prediction'])\n",
    "        \n",
    "        # Historical prices (window)\n",
    "        hist_prices = []\n",
    "        if 'recent_prices' in self.data.columns and self.data.iloc[idx]['recent_prices'] is not None:\n",
    "            hist_prices = list(self.data.iloc[idx]['recent_prices'])\n",
    "        if len(hist_prices) < self.window_size:\n",
    "            pad_value = hist_prices[-1] if hist_prices else llm_pred\n",
    "            hist_prices = hist_prices + [pad_value] * (self.window_size - len(hist_prices))\n",
    "        hist_prices = np.array(hist_prices[-self.window_size:], dtype=np.float32)\n",
    "        \n",
    "        # Volatility\n",
    "        volatility = calculate_volatility(hist_prices)\n",
    "        \n",
    "        # VaR (using historical returns)\n",
    "        returns = np.diff(hist_prices) / hist_prices[:-1] if len(hist_prices) > 1 else np.array([0.0])\n",
    "        var = calculate_var(returns, self.confidence_level)\n",
    "        \n",
    "        # Justification-driven features\n",
    "        extra_features = [\n",
    "            float(self.data.iloc[idx].get(col, 0.0))\n",
    "            for col in self.available_extra_cols\n",
    "        ]\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            np.array([llm_pred], dtype=np.float32),\n",
    "            hist_prices,\n",
    "            np.array([volatility, var], dtype=np.float32),\n",
    "            np.array(extra_features, dtype=np.float32)\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step\"\"\"\n",
    "        idx = min(self.current_step, self.max_steps - 1)\n",
    "        \n",
    "        # Get LLM prediction and actual price\n",
    "        llm_pred = float(self.data.iloc[idx]['llm_prediction'])\n",
    "        actual_price = float(self.data.iloc[idx]['actual_price'])\n",
    "        \n",
    "        # Apply action (adjustment)\n",
    "        adjustment = float(action[0])\n",
    "        adjusted_pred = llm_pred * (1 + adjustment)\n",
    "        \n",
    "        # Calculate prediction error (relative if possible)\n",
    "        pred_error = abs(adjusted_pred - actual_price)\n",
    "        if actual_price != 0:\n",
    "            scaled_error = pred_error / abs(actual_price)\n",
    "        else:\n",
    "            scaled_error = pred_error\n",
    "        \n",
    "        # Calculate risk penalty (using CVaR)\n",
    "        if 'recent_prices' in self.data.columns and self.data.iloc[idx]['recent_prices'] is not None:\n",
    "            hist_prices = np.array(self.data.iloc[idx]['recent_prices'][-self.window_size:], dtype=np.float32)\n",
    "            returns = np.diff(hist_prices) / hist_prices[:-1] if len(hist_prices) > 1 else np.array([0.0])\n",
    "            cvar = abs(calculate_cvar(returns, self.confidence_level))\n",
    "        else:\n",
    "            cvar = 0.0\n",
    "        \n",
    "        confidence = float(self.data.iloc[idx].get('llm_likelihood', 0.5))\n",
    "        justification_risk = float(self.data.iloc[idx].get('justification_risk_ratio', 0.0))\n",
    "        justification_polarity = float(self.data.iloc[idx].get('justification_polarity', 0.0))\n",
    "        \n",
    "        risk_penalty = self.lambda_risk * cvar\n",
    "        confidence_penalty = self.confidence_penalty_weight * (1 - confidence)\n",
    "        justification_penalty = self.justification_weight * justification_risk\n",
    "        sentiment_penalty = self.sentiment_weight * max(-justification_polarity, 0.0)\n",
    "        \n",
    "        reward = -scaled_error - risk_penalty - confidence_penalty - justification_penalty - sentiment_penalty\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= self.max_steps\n",
    "        truncated = False\n",
    "        \n",
    "        # Next observation\n",
    "        next_state = self._get_observation()\n",
    "        \n",
    "        return next_state, reward, terminated, truncated, {}\n",
    "\n",
    "print(\"Stock Prediction Environment defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c42bea",
   "metadata": {},
   "source": [
    "## 6. PPO Training on Training Data\n",
    "\n",
    "Train the PPO agent on the training set to learn risk-aware adjustments to LLM predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for PPO using training set with LLM predictions\n",
    "train_parsed = []\n",
    "for idx, item in enumerate(train_data):\n",
    "    parsed = parse_prompt_data(item['prompt'])\n",
    "    response = json.loads(item['response'])\n",
    "    llm_output = train_llm_results[idx] if idx < len(train_llm_results) else {}\n",
    "\n",
    "    if isinstance(llm_output, dict) and llm_output.get('predicted_close') is not None:\n",
    "        parsed['llm_prediction'] = safe_float(llm_output.get('predicted_close'), response['predicted_close'])\n",
    "    elif idx < len(train_llm_predictions):\n",
    "        parsed['llm_prediction'] = train_llm_predictions[idx]\n",
    "    else:\n",
    "        parsed['llm_prediction'] = response['predicted_close']\n",
    "\n",
    "    if idx < len(train_actual_prices):\n",
    "        parsed['actual_price'] = train_actual_prices[idx]\n",
    "    else:\n",
    "        parsed['actual_price'] = response['predicted_close']\n",
    "\n",
    "    llm_likelihood = safe_float(llm_output.get('likelihood') if isinstance(llm_output, dict) else None, response.get('likelihood', 0.5))\n",
    "    parsed['llm_likelihood'] = llm_likelihood\n",
    "    parsed['likelihood'] = llm_likelihood\n",
    "\n",
    "    justification_text = llm_output.get('justification', '') if isinstance(llm_output, dict) else ''\n",
    "    parsed['llm_justification'] = justification_text\n",
    "    parsed.update(extract_justification_features(justification_text))\n",
    "\n",
    "    train_parsed.append(parsed)\n",
    "\n",
    "train_df_ppo = pd.DataFrame(train_parsed)\n",
    "print(f\"Training data prepared for PPO training: {len(train_df_ppo)} samples\")\n",
    "print(f\"With LLM predictions: {sum(train_df_ppo['llm_prediction'].notna())} samples\")\n",
    "train_df_ppo.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa77eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train PPO model\n",
    "print(\"Creating PPO training environment...\")\n",
    "\n",
    "# Create environment using TRAINING data (more samples = better RL learning)\n",
    "env = StockPredictionEnv(train_df_ppo, window_size=5)\n",
    "\n",
    "# Initialize PPO agent\n",
    "print(\"\\nInitializing PPO agent...\")\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train PPO model on training data\n",
    "print(\"\\nTraining PPO model on training data...\")\n",
    "print(f\"Training samples: {len(train_df_ppo)}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Adjust total_timesteps based on training data size\n",
    "# Using more timesteps for larger training set\n",
    "total_timesteps = min(200000, len(train_df_ppo) * 20)\n",
    "print(f\"Total timesteps: {total_timesteps}\")\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "print(\"\\n‚úÖ PPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e3a4e",
   "metadata": {},
   "source": [
    "### 6.1 (Optional) Validate PPO on Validation Set\n",
    "\n",
    "Before applying to test data, optionally evaluate PPO performance on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Prepare and evaluate on validation data\n",
    "val_parsed = []\n",
    "for idx, item in enumerate(val_data):\n",
    "    parsed = parse_prompt_data(item['prompt'])\n",
    "    response = json.loads(item['response'])\n",
    "    llm_output = val_llm_results[idx] if idx < len(val_llm_results) else {}\n",
    "\n",
    "    if isinstance(llm_output, dict) and llm_output.get('predicted_close') is not None:\n",
    "        parsed['llm_prediction'] = safe_float(llm_output.get('predicted_close'), response['predicted_close'])\n",
    "    elif idx < len(val_llm_predictions):\n",
    "        parsed['llm_prediction'] = val_llm_predictions[idx]\n",
    "    else:\n",
    "        parsed['llm_prediction'] = response['predicted_close']\n",
    "\n",
    "    if idx < len(val_actual_prices):\n",
    "        parsed['actual_price'] = val_actual_prices[idx]\n",
    "    else:\n",
    "        parsed['actual_price'] = response['predicted_close']\n",
    "\n",
    "    llm_likelihood = safe_float(llm_output.get('likelihood') if isinstance(llm_output, dict) else None, response.get('likelihood', 0.5))\n",
    "    parsed['llm_likelihood'] = llm_likelihood\n",
    "    parsed['likelihood'] = llm_likelihood\n",
    "\n",
    "    justification_text = llm_output.get('justification', '') if isinstance(llm_output, dict) else ''\n",
    "    parsed['llm_justification'] = justification_text\n",
    "    parsed.update(extract_justification_features(justification_text))\n",
    "\n",
    "    val_parsed.append(parsed)\n",
    "\n",
    "val_df = pd.DataFrame(val_parsed)\n",
    "\n",
    "# Apply PPO to validation set\n",
    "val_env = StockPredictionEnv(val_df, window_size=5)\n",
    "val_obs, _ = val_env.reset()\n",
    "\n",
    "val_ppo_predictions = []\n",
    "for idx in range(len(val_df)):\n",
    "    if idx < val_env.window_size:\n",
    "        val_ppo_predictions.append(val_df.iloc[idx]['llm_prediction'])\n",
    "        continue\n",
    "    \n",
    "    action, _ = model.predict(val_obs, deterministic=True)\n",
    "    llm_pred = val_df.iloc[idx]['llm_prediction']\n",
    "    adjusted_pred = llm_pred * (1 + action[0])\n",
    "    val_ppo_predictions.append(adjusted_pred)\n",
    "    \n",
    "    if idx < len(val_df) - 1:\n",
    "        val_obs, _, terminated, _, _ = val_env.step(action)\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "val_df['ppo_adjusted_prediction'] = val_ppo_predictions\n",
    "\n",
    "# Quick validation metrics\n",
    "val_llm_mae = np.mean(np.abs(val_df['llm_prediction'] - val_df['actual_price']))\n",
    "val_ppo_mae = np.mean(np.abs(val_df['ppo_adjusted_prediction'] - val_df['actual_price']))\n",
    "\n",
    "print(f\"\n",
    "Validation Set Results:\")\n",
    "print(f\"LLM MAE: {val_llm_mae:.4f}\")\n",
    "print(f\"LLM-PPO MAE: {val_ppo_mae:.4f}\")\n",
    "print(f\"Improvement: {((val_llm_mae - val_ppo_mae) / val_llm_mae * 100):.2f}%\")\n",
    "print(\"\n",
    "‚úÖ Validation complete! Proceeding to test set...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb96aee",
   "metadata": {},
   "source": [
    "## 7. Apply PPO Adjustments to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e20e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PPO adjustments to test predictions\n",
    "def apply_ppo_adjustment(model, test_df):\n",
    "    \"\"\"Apply trained PPO model to adjust predictions\"\"\"\n",
    "    adjusted_predictions = []\n",
    "    \n",
    "    env = StockPredictionEnv(test_df, window_size=5)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    for idx in range(len(test_df)):\n",
    "        if idx < env.window_size:\n",
    "            # For early samples, use LLM prediction as-is\n",
    "            adjusted_predictions.append(test_df.iloc[idx]['llm_prediction'])\n",
    "            continue\n",
    "        \n",
    "        # Get PPO action\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Apply adjustment\n",
    "        llm_pred = test_df.iloc[idx]['llm_prediction']\n",
    "        adjusted_pred = llm_pred * (1 + action[0])\n",
    "        adjusted_predictions.append(adjusted_pred)\n",
    "        \n",
    "        # Step environment\n",
    "        if idx < len(test_df) - 1:\n",
    "            obs, _, terminated, _, _ = env.step(action)\n",
    "            if terminated:\n",
    "                break\n",
    "    \n",
    "    return adjusted_predictions\n",
    "\n",
    "print(\"Applying PPO adjustments to test set...\")\n",
    "test_df['ppo_adjusted_prediction'] = apply_ppo_adjustment(model, test_df)\n",
    "print(\"PPO adjustments applied!\")\n",
    "\n",
    "# Display results\n",
    "test_df[['ticker', 'llm_prediction', 'ppo_adjusted_prediction', 'actual_price']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c90a6",
   "metadata": {},
   "source": [
    "## 8. Baseline Models Implementation (COMMENTED OUT - Only using LLM and LLM-PPO)\n",
    "\n",
    "<!-- Baseline models (SVR, XGBoost, LSTM) are commented out to focus on LLM and LLM-PPO comparison -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare features from all_labels for baseline models\n",
    "# # Filter for test period (last 30% of data)\n",
    "# all_labels['Date'] = pd.to_datetime(all_labels['Date'])\n",
    "# all_labels = all_labels.sort_values('Date')\n",
    "\n",
    "# # Create feature set\n",
    "# feature_cols = ['SMA_20', 'SMA_50', 'EMA_12', 'EMA_26', 'RSI_14', \n",
    "#                 'MACD', 'MACD_signal', 'MACD_hist', 'BB_width_20_2',\n",
    "#                 'headline_count', 'sent_compound_mean']\n",
    "\n",
    "# # Fill NaN values\n",
    "# all_labels[feature_cols] = all_labels[feature_cols].fillna(0)\n",
    "\n",
    "# # Split by date (70% train, 30% test)\n",
    "# train_size = int(len(all_labels) * 0.7)\n",
    "# train_labels = all_labels.iloc[:train_size]\n",
    "# test_labels = all_labels.iloc[train_size:]\n",
    "\n",
    "# X_train = train_labels[feature_cols].values\n",
    "# y_train = train_labels['next_close'].values\n",
    "# X_test = test_labels[feature_cols].values\n",
    "# y_test = test_labels['next_close'].values\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print(f\"Training set: {X_train.shape}\")\n",
    "# print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "print(\"Baseline models commented out - only using LLM and LLM-PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d119bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train SVR model\n",
    "# print(\"Training SVR model...\")\n",
    "# svr_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "# svr_model.fit(X_train_scaled, y_train)\n",
    "# svr_predictions = svr_model.predict(X_test_scaled)\n",
    "# print(\"SVR training completed!\")\n",
    "\n",
    "print(\"SVR model commented out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train XGBoost model\n",
    "# print(\"Training XGBoost model...\")\n",
    "# xgb_model = XGBRegressor(\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=5,\n",
    "#     random_state=42\n",
    "# )\n",
    "# xgb_model.fit(X_train_scaled, y_train)\n",
    "# xgb_predictions = xgb_model.predict(X_test_scaled)\n",
    "# print(\"XGBoost training completed!\")\n",
    "\n",
    "print(\"XGBoost model commented out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f0d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build LSTM model\n",
    "# print(\"Building and training LSTM model...\")\n",
    "\n",
    "# # Reshape data for LSTM (samples, timesteps, features)\n",
    "# X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "# X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# # Build LSTM model\n",
    "# lstm_model = Sequential([\n",
    "#     LSTM(50, activation='relu', input_shape=(1, X_train_scaled.shape[1])),\n",
    "#     Dense(25, activation='relu'),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "\n",
    "# lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train LSTM\n",
    "# history = lstm_model.fit(\n",
    "#     X_train_lstm, \n",
    "#     y_train,\n",
    "#     epochs=50,\n",
    "#     batch_size=32,\n",
    "#     validation_split=0.1,\n",
    "#     verbose=0\n",
    "# )\n",
    "\n",
    "# lstm_predictions = lstm_model.predict(X_test_lstm).flatten()\n",
    "# print(\"LSTM training completed!\")\n",
    "\n",
    "print(\"LSTM model commented out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e201f",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22226ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric functions\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Square Error\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def calculate_returns(prices):\n",
    "    \"\"\"Calculate returns from prices\"\"\"\n",
    "    prices = np.array(prices)\n",
    "    return np.diff(prices) / prices[:-1]\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.0):\n",
    "    \"\"\"Calculate Sharpe Ratio\"\"\"\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    if np.std(returns) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(excess_returns) / np.std(returns)\n",
    "\n",
    "def calculate_sortino_ratio(returns, risk_free_rate=0.0):\n",
    "    \"\"\"Calculate Sortino Ratio\"\"\"\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    downside_returns = returns[returns < 0]\n",
    "    if len(downside_returns) == 0 or np.std(downside_returns) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(excess_returns) / np.std(downside_returns)\n",
    "\n",
    "def calculate_max_drawdown(prices):\n",
    "    \"\"\"Calculate Maximum Drawdown\"\"\"\n",
    "    prices = np.array(prices)\n",
    "    cummax = np.maximum.accumulate(prices)\n",
    "    drawdowns = (prices - cummax) / cummax\n",
    "    return np.min(drawdowns)\n",
    "\n",
    "def calculate_cumulative_return(prices):\n",
    "    \"\"\"Calculate Cumulative Return\"\"\"\n",
    "    prices = np.array(prices)\n",
    "    return (prices[-1] - prices[0]) / prices[0]\n",
    "\n",
    "print(\"Evaluation metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models by ticker\n",
    "def evaluate_model_by_ticker(predictions, actual_prices, test_labels):\n",
    "    \"\"\"Evaluate model performance for each ticker\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for ticker in test_labels['ticker'].unique():\n",
    "        ticker_mask = test_labels['ticker'] == ticker\n",
    "        ticker_pred = predictions[ticker_mask]\n",
    "        ticker_actual = actual_prices[ticker_mask]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mape = calculate_mape(ticker_actual, ticker_pred)\n",
    "        rmse = calculate_rmse(ticker_actual, ticker_pred)\n",
    "        \n",
    "        # Returns-based metrics\n",
    "        returns = calculate_returns(ticker_pred)\n",
    "        sharpe = calculate_sharpe_ratio(returns)\n",
    "        sortino = calculate_sortino_ratio(returns)\n",
    "        max_dd = calculate_max_drawdown(ticker_pred)\n",
    "        cum_return = calculate_cumulative_return(ticker_pred)\n",
    "        \n",
    "        results[ticker] = {\n",
    "            'MAPE': mape,\n",
    "            'RMSE': rmse,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Sortino Ratio': sortino,\n",
    "            'Max Drawdown': max_dd,\n",
    "            'Cumulative Return': cum_return\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Model evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b801f7",
   "metadata": {},
   "source": [
    "## 10. Results Comparison and Analysis (LLM vs LLM-PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced07683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all model predictions\n",
    "models_results = {}\n",
    "\n",
    "# # Baseline models (COMMENTED OUT)\n",
    "# models_results['SVR'] = evaluate_model_by_ticker(svr_predictions, y_test, test_labels)\n",
    "# models_results['XGBoost'] = evaluate_model_by_ticker(xgb_predictions, y_test, test_labels)\n",
    "# models_results['LSTM'] = evaluate_model_by_ticker(lstm_predictions, y_test, test_labels)\n",
    "\n",
    "# For LLM and LLM-PPO, we need to evaluate from test_df\n",
    "# Evaluate LLM predictions\n",
    "if 'llm_prediction' in test_df.columns:\n",
    "    llm_predictions = test_df['llm_prediction'].values\n",
    "    actual_prices = test_df['actual_price'].values\n",
    "    models_results['LLM'] = evaluate_model_by_ticker(llm_predictions, actual_prices, test_df)\n",
    "\n",
    "# Evaluate LLM-PPO predictions\n",
    "if 'ppo_adjusted_prediction' in test_df.columns:\n",
    "    ppo_predictions = test_df['ppo_adjusted_prediction'].values\n",
    "    actual_prices = test_df['actual_price'].values\n",
    "    models_results['LLM-PPO'] = evaluate_model_by_ticker(ppo_predictions, actual_prices, test_df)\n",
    "\n",
    "print(\"Model evaluation completed!\")\n",
    "print(f\"\\nNumber of models evaluated: {len(models_results)}\")\n",
    "print(f\"Models: {list(models_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "def create_comparison_table(models_results):\n",
    "    \"\"\"Create a comprehensive comparison table\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, ticker_results in models_results.items():\n",
    "        for ticker, metrics in ticker_results.items():\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Ticker': ticker,\n",
    "                **metrics\n",
    "            }\n",
    "            comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "comparison_df = create_comparison_table(models_results)\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics across all tickers\n",
    "avg_metrics = comparison_df.groupby('Model')[['MAPE', 'RMSE', 'Sharpe Ratio', \n",
    "                                                'Sortino Ratio', 'Max Drawdown', \n",
    "                                                'Cumulative Return']].mean()\n",
    "\n",
    "print(\"\\nAverage Performance Across All Tickers:\")\n",
    "avg_metrics.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406d4ba",
   "metadata": {},
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b274ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAPE comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "comparison_df_pivot = comparison_df.pivot(index='Ticker', columns='Model', values='MAPE')\n",
    "comparison_df_pivot.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('MAPE Comparison by Ticker', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ticker')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "avg_metrics['MAPE'].plot(kind='bar', color='steelblue')\n",
    "plt.title('Average MAPE Across All Tickers', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4024d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "comparison_df_pivot = comparison_df.pivot(index='Ticker', columns='Model', values='RMSE')\n",
    "comparison_df_pivot.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('RMSE Comparison by Ticker', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ticker')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "avg_metrics['RMSE'].plot(kind='bar', color='coral')\n",
    "plt.title('Average RMSE Across All Tickers', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c912d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot risk-adjusted metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Sharpe Ratio\n",
    "avg_metrics['Sharpe Ratio'].plot(kind='bar', ax=axes[0, 0], color='green', alpha=0.7)\n",
    "axes[0, 0].set_title('Sharpe Ratio (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('Sharpe Ratio')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sortino Ratio\n",
    "avg_metrics['Sortino Ratio'].plot(kind='bar', ax=axes[0, 1], color='blue', alpha=0.7)\n",
    "axes[0, 1].set_title('Sortino Ratio (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Sortino Ratio')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Maximum Drawdown\n",
    "avg_metrics['Max Drawdown'].plot(kind='bar', ax=axes[1, 0], color='red', alpha=0.7)\n",
    "axes[1, 0].set_title('Maximum Drawdown (Closer to 0 is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Model')\n",
    "axes[1, 0].set_ylabel('Max Drawdown')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative Return\n",
    "avg_metrics['Cumulative Return'].plot(kind='bar', ax=axes[1, 1], color='purple', alpha=0.7)\n",
    "axes[1, 1].set_title('Cumulative Return (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_ylabel('Cumulative Return')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Risk-Adjusted Performance Metrics', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23903077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction visualization (if test_df available)\n",
    "if 'ticker' in test_df.columns:\n",
    "    # Select one ticker for detailed visualization\n",
    "    sample_ticker = test_df['ticker'].iloc[0]\n",
    "    ticker_data = test_df[test_df['ticker'] == sample_ticker].head(50)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    x = range(len(ticker_data))\n",
    "    plt.plot(x, ticker_data['actual_price'].values, 'ko-', label='Actual Price', linewidth=2, markersize=6)\n",
    "    plt.plot(x, ticker_data['llm_prediction'].values, 'bs--', label='LLM Prediction', linewidth=1.5, markersize=5, alpha=0.7)\n",
    "    \n",
    "    if 'ppo_adjusted_prediction' in ticker_data.columns:\n",
    "        plt.plot(x, ticker_data['ppo_adjusted_prediction'].values, 'r^--', label='LLM-PPO Prediction', linewidth=1.5, markersize=5, alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Stock Price Predictions for {sample_ticker} (First 50 Test Samples)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e9e18",
   "metadata": {},
   "source": [
    "## 12. Key Findings and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. PREDICTION ACCURACY (Lower is Better)\")\n",
    "print(\"-\" * 80)\n",
    "accuracy_summary = avg_metrics[['MAPE', 'RMSE']].round(4)\n",
    "print(accuracy_summary)\n",
    "\n",
    "print(\"\\n2. RISK-ADJUSTED RETURNS (Higher is Better for Ratios)\")\n",
    "print(\"-\" * 80)\n",
    "risk_summary = avg_metrics[['Sharpe Ratio', 'Sortino Ratio']].round(4)\n",
    "print(risk_summary)\n",
    "\n",
    "print(\"\\n3. RISK METRICS\")\n",
    "print(\"-\" * 80)\n",
    "drawdown_summary = avg_metrics[['Max Drawdown', 'Cumulative Return']].round(4)\n",
    "print(drawdown_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "The two-stage LLM-PPO framework aims to:\n",
    "1. Generate initial predictions using LLM with historical data and sentiment\n",
    "2. Refine predictions using PPO with risk-aware adjustments (VaR, CVaR)\n",
    "\n",
    "Key Benefits:\n",
    "- Incorporates both market data and qualitative information (news sentiment)\n",
    "- Balances prediction accuracy with financial risk management\n",
    "- Provides more stable predictions compared to pure ML/DL approaches\n",
    "- Better risk-adjusted returns through CVaR-based reward function\n",
    "\n",
    "The framework demonstrates the potential of combining LLMs with reinforcement\n",
    "learning for robust financial forecasting in uncertain market environments.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7ed89",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "output_dir = '../results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(f'{output_dir}/model_comparison_results.csv', index=False)\n",
    "print(f\"Comparison results saved to {output_dir}/model_comparison_results.csv\")\n",
    "\n",
    "# Save average metrics\n",
    "avg_metrics.to_csv(f'{output_dir}/average_metrics.csv')\n",
    "print(f\"Average metrics saved to {output_dir}/average_metrics.csv\")\n",
    "\n",
    "# Save PPO model\n",
    "model.save(f'{output_dir}/ppo_stock_prediction_model')\n",
    "print(f\"PPO model saved to {output_dir}/ppo_stock_prediction_model\")\n",
    "\n",
    "# Save test predictions\n",
    "if 'ppo_adjusted_prediction' in test_df.columns:\n",
    "    test_df.to_csv(f'{output_dir}/test_predictions.csv', index=False)\n",
    "    print(f\"Test predictions saved to {output_dir}/test_predictions.csv\")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e06a9",
   "metadata": {},
   "source": [
    "## 14. Next Steps and Extensions\n",
    "\n",
    "### Potential Improvements:\n",
    "1. **Fine-tune LLM**: Fine-tune the Llama model on financial data for better domain-specific predictions\n",
    "2. **Enhanced PPO**: Experiment with different reward functions and hyperparameters\n",
    "3. **More Baselines**: Implement TCN (Temporal Convolutional Network) for comparison\n",
    "4. **Real-time Prediction**: Adapt the framework for real-time stock prediction\n",
    "5. **Portfolio Optimization**: Extend to multi-stock portfolio management\n",
    "6. **Risk Metrics**: Incorporate additional risk metrics (CVaR at different confidence levels)\n",
    "7. **Ensemble Methods**: Combine multiple models for more robust predictions\n",
    "8. **Market Regime Detection**: Adapt strategy based on market conditions (bull/bear markets)\n",
    "\n",
    "### Research Directions:\n",
    "- Study the interpretability of LLM predictions\n",
    "- Analyze the impact of different sentiment sources\n",
    "- Investigate transfer learning across different stocks\n",
    "- Explore attention mechanisms in the PPO policy network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
