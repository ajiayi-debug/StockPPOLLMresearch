{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fb81b9",
   "metadata": {},
   "source": [
    "# Generate Justifications and Q&A with OpenAI Batch API\n",
    "\n",
    "This notebook processes your stock prediction data using **OpenAI's Batch API**. This approach avoids rate-limiting issues and is highly efficient for large datasets.\n",
    "\n",
    "## Workflow Overview:\n",
    "\n",
    "**RECOMMENDED: Use the Async Workflow (Sections 5A-5C)**\n",
    "1. **Section 5A**: Submit all batches at once (non-blocking)\n",
    "2. Wait hours/day while OpenAI processes\n",
    "3. **Section 5B**: Check status periodically\n",
    "4. **Section 5C**: Retrieve all completed results\n",
    "5. **Section 6**: Save augmented data\n",
    "\n",
    "**Alternative: Use Section 5 for Blocking Workflow** (waits for each batch sequentially - slower)\n",
    "\n",
    "**Note**: Batch jobs are processed by OpenAI within a 24-hour window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42508eb9",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce00d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "BATCH_INPUT_DIR = \"batch_inputs\"\n",
    "os.makedirs(BATCH_INPUT_DIR, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "TRAIN_FILE = \"../finetune_paper/train.jsonl\"\n",
    "VAL_FILE = \"../finetune_paper/val.jsonl\"\n",
    "TEST_FILE = \"../finetune_paper/test.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea3f0c",
   "metadata": {},
   "source": [
    "## 2. Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509e8dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loaded 8698 train, 1243 val, 2477 test samples\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "print(\"Loading datasets...\")\n",
    "train_data = load_jsonl(TRAIN_FILE)\n",
    "val_data = load_jsonl(VAL_FILE)\n",
    "test_data = load_jsonl(TEST_FILE)\n",
    "print(f\"Loaded {len(train_data)} train, {len(val_data)} val, {len(test_data)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd518681",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2095e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_file(data, task_type, output_file, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Prepare batch file for OpenAI Batch API (with chunking support)\n",
    "    \n",
    "    Args:\n",
    "        data: List of samples\n",
    "        task_type: \"justification\" or \"qa_chain\"\n",
    "        output_file: Path to save batch input file\n",
    "        chunk_size: Max samples per file (default 1000 to stay under token limit)\n",
    "    \n",
    "    Returns:\n",
    "        List of created batch file paths\n",
    "    \"\"\"\n",
    "    batch_requests = []\n",
    "    \n",
    "    for idx, sample in enumerate(data):\n",
    "        # Get the prompt and response from the sample\n",
    "        prompt = sample[\"prompt\"]\n",
    "        response = sample[\"response\"]\n",
    "        \n",
    "        if task_type == \"justification\":\n",
    "            system_prompt = \"You are a financial analyst who provides clear, concise explanations for stock price predictions.\"\n",
    "            user_prompt = f\"{prompt}\\n\\nThe predicted answer is: {response}\\n\\nProvide a 2-3 sentence justification explaining WHY this prediction makes sense based on the indicators and sentiment provided in the context above.\"\n",
    "            response_format = None\n",
    "        else:  # qa_chain\n",
    "            system_prompt = \"You are a financial analyst who helps explain stock predictions through Q&A.\"\n",
    "            user_prompt = f\"{prompt}\\n\\nThe predicted answer is: {response}\\n\\nGenerate 3-4 question-answer pairs that help explain the reasoning behind this prediction based on the market data provided. Return as JSON with format: {{\\\"qa_pairs\\\": [{{\\\"question\\\": \\\"...\\\", \\\"answer\\\": \\\"...\\\"}}]}}\"\n",
    "            response_format = {\"type\": \"json_object\"}\n",
    "\n",
    "        print(system_prompt)\n",
    "        print(user_prompt)\n",
    "        \n",
    "        request = {\n",
    "            \"custom_id\": f\"{task_type}_{idx}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                \"max_tokens\": 500\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if response_format:\n",
    "            request[\"body\"][\"response_format\"] = response_format\n",
    "        \n",
    "        batch_requests.append(request)\n",
    "    \n",
    "    # Split into chunks and write multiple files if needed\n",
    "    created_files = []\n",
    "    num_chunks = (len(batch_requests) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, len(batch_requests))\n",
    "        chunk = batch_requests[start_idx:end_idx]\n",
    "        \n",
    "        # Create chunk filename\n",
    "        if num_chunks > 1:\n",
    "            base_name = output_file.replace('.jsonl', f'_chunk{chunk_idx+1}.jsonl')\n",
    "        else:\n",
    "            base_name = output_file\n",
    "        \n",
    "        # Write chunk to file\n",
    "        with open(base_name, 'w') as f:\n",
    "            for request in chunk:\n",
    "                f.write(json.dumps(request) + '\\n')\n",
    "        \n",
    "        print(f\"Created batch file: {base_name} with {len(chunk)} requests\")\n",
    "        created_files.append(base_name)\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "\n",
    "def upload_file_and_wait(file_path):\n",
    "    \"\"\"Upload file to OpenAI\"\"\"\n",
    "    print(f\"Uploading {file_path}...\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        batch_input_file = client.files.create(file=f, purpose=\"batch\")\n",
    "    print(f\"Uploaded file ID: {batch_input_file.id}\")\n",
    "    return batch_input_file.id\n",
    "\n",
    "\n",
    "def retrieve_existing_batches():\n",
    "    \"\"\"\n",
    "    Retrieve all existing batch jobs and organize by description\n",
    "    Returns dict mapping description to batch info\n",
    "    \"\"\"\n",
    "    batches = client.batches.list(limit=100)\n",
    "    \n",
    "    existing = {}\n",
    "    for batch in batches.data:\n",
    "        desc = batch.metadata.get('description', '')\n",
    "        if desc:\n",
    "            existing[desc] = {\n",
    "                'id': batch.id,\n",
    "                'status': batch.status,\n",
    "                'output_file_id': batch.output_file_id if batch.status == 'completed' else None,\n",
    "                'request_counts': batch.request_counts\n",
    "            }\n",
    "    \n",
    "    return existing\n",
    "\n",
    "\n",
    "def process_results(output_file_id, original_data):\n",
    "    \"\"\"Download and process batch results\"\"\"\n",
    "    print(f\"\\nDownloading results from {output_file_id}...\")\n",
    "    \n",
    "    # Download the output file\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    results = []\n",
    "    \n",
    "    for line in file_response.text.strip().split('\\n'):\n",
    "        results.append(json.loads(line))\n",
    "    \n",
    "    # Sort by custom_id to match original order\n",
    "    # Handle both \"justification_123\" and \"qa_chain_123\" formats\n",
    "    results.sort(key=lambda x: int(x['custom_id'].split('_')[-1]))\n",
    "    \n",
    "    # Extract the generated content\n",
    "    augmented_data = []\n",
    "    for i, result in enumerate(results):\n",
    "        original_sample = original_data[i]\n",
    "        response_content = result['response']['body']['choices'][0]['message']['content']\n",
    "        \n",
    "        augmented_sample = original_sample.copy()\n",
    "        augmented_sample['generated_content'] = response_content\n",
    "        augmented_data.append(augmented_sample)\n",
    "    \n",
    "    print(f\"Processed {len(augmented_data)} results\")\n",
    "    return augmented_data\n",
    "\n",
    "def generate_sample_prompts(data, num_samples=2):\n",
    "    \"\"\"\n",
    "    Generate sample prompts to show what gets sent to GPT-4o\n",
    "    \"\"\"\n",
    "    \n",
    "    for idx in range(min(num_samples, len(data))):\n",
    "        sample = data[idx]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        response = sample[\"response\"]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SAMPLE {idx + 1}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # ==================== JUSTIFICATION TASK ====================\n",
    "        print(\" TASK: JUSTIFICATION GENERATION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        system_prompt_just = \"You are a financial analyst who provides clear, concise explanations for stock price predictions.\"\n",
    "        user_prompt_just = f\"{prompt}\\n\\nThe predicted answer is: {response}\\n\\nProvide a 2-3 sentence justification explaining WHY this prediction makes sense based on the indicators and sentiment provided in the context above.\"\n",
    "        \n",
    "        print(\"\\n SYSTEM PROMPT:\")\n",
    "        print(system_prompt_just)\n",
    "        \n",
    "        print(\"\\n USER PROMPT:\")\n",
    "        print(user_prompt_just)\n",
    "        \n",
    "        print(\"\\n FULL API REQUEST BODY:\")\n",
    "        request_body_just = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt_just},\n",
    "                {\"role\": \"user\", \"content\": user_prompt_just}\n",
    "            ],\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "        print(json.dumps(request_body_just, indent=2)[:1000] + \"...\" if len(json.dumps(request_body_just, indent=2)) > 1000 else json.dumps(request_body_just, indent=2))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # ==================== Q&A TASK ====================\n",
    "        print(\" TASK: Q&A CHAIN GENERATION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        system_prompt_qa = \"You are a financial analyst who helps explain stock predictions through Q&A.\"\n",
    "        user_prompt_qa = f\"{prompt}\\n\\nThe predicted answer is: {response}\\n\\nGenerate 3-4 question-answer pairs that help explain the reasoning behind this prediction based on the market data provided. Return as JSON with format: {{\\\"qa_pairs\\\": [{{\\\"question\\\": \\\"...\\\", \\\"answer\\\": \\\"...\\\"}}]}}\"\n",
    "        \n",
    "        print(\"\\n SYSTEM PROMPT:\")\n",
    "        print(system_prompt_qa)\n",
    "        \n",
    "        print(\"\\n USER PROMPT:\")\n",
    "        print(user_prompt_qa)\n",
    "        \n",
    "        print(\"\\n FULL API REQUEST BODY:\")\n",
    "        request_body_qa = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt_qa},\n",
    "                {\"role\": \"user\", \"content\": user_prompt_qa}\n",
    "            ],\n",
    "            \"max_tokens\": 500,\n",
    "            \"response_format\": {\"type\": \"json_object\"}\n",
    "        }\n",
    "        print(json.dumps(request_body_qa, indent=2)[:1000] + \"...\" if len(json.dumps(request_body_qa, indent=2)) > 1000 else json.dumps(request_body_qa, indent=2))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e9cb1",
   "metadata": {},
   "source": [
    "# Sample prompts (visible in finetune_ablations/batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95137e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating Sample Prompts for GPT-4o\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1\n",
      "================================================================================\n",
      "\n",
      " TASK: JUSTIFICATION GENERATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " SYSTEM PROMPT:\n",
      "You are a financial analyst who provides clear, concise explanations for stock price predictions.\n",
      "\n",
      " USER PROMPT:\n",
      "You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-justified, considering multiple financial factors.\n",
      "\n",
      "‚Ä¢ Predicted Stock Price: The forecasted close price for the next trading day.\n",
      "‚Ä¢ Price Movement Likelihood: The likelihood of the predicted stock price.\n",
      "‚Ä¢ Justification: Provide an explanation for the predicted stock price and the corresponding likelihood, considering the following:\n",
      "  - Historical market data (e.g., recent closing prices).\n",
      "  - Technical indicators (e.g., SMA, EMA, RSI, MACD, Bollinger Bands).\n",
      "  - Sentiment analysis (e.g., news sentiment, market sentiment).\n",
      "\n",
      "Please weigh these signals and justify the predicted stock price.\n",
      "\n",
      "TICKER: HSBC\n",
      "DATE: 2023-01-03\n",
      "\n",
      "RECENT CLOSING PRICES (most recent last): 31.0700, 31.0300, 31.2100, 31.1600, 31.6300\n",
      "\n",
      "TECHNICAL INDICATORS:\n",
      "SMA_20=30.614999866485597, SMA_50=29.027199935913085,\n",
      "EMA_12=30.909996996764857, EMA_26=30.33953977919106,\n",
      "RSI_14=70.01903430263613, MACD=0.5704572175737965, MACD_signal=0.5609859479794548, MACD_hist=0.0094712695943417,\n",
      "BB_width_20_2=0.0625429400529223\n",
      "\n",
      "SENTIMENT AGGREGATES:\n",
      "headline_count=4.0, sent_compound_mean=0.072325\n",
      "\n",
      "HEADLINES (concise):\n",
      "BP, Unilever, and HSBC have failed to properly exit Russia after Ukraine war, new report warns - City AM | Rising interest rates to boost HSBC profit amid uncertain economic outlook - South China Morning Post | HSBC connects virtual and real world with seasonal campaign - Marketing-Interactive | John Lloyd leaves HSBC - Asset Servicing Times\n",
      "\n",
      "Return STRICT JSON with keys:\n",
      "- predicted_close (float, next-day close price),\n",
      "- likelihood (float in [0,1]),\n",
      "- justification (string, 1‚Äì2 sentences).\n",
      "JSON:\n",
      "\n",
      "The predicted answer is: {\"predicted_close\": 32.68000030517578, \"likelihood\": 0.9, \"justification\": \"n/a\"}\n",
      "\n",
      "Provide a 2-3 sentence justification explaining WHY this prediction makes sense based on the indicators and sentiment provided in the context above.\n",
      "\n",
      " FULL API REQUEST BODY:\n",
      "{\n",
      "  \"model\": \"gpt-4o-mini\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a financial analyst who provides clear, concise explanations for stock price predictions.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"You are a financial analyst with expertise in stock market forecasting.\\nYour task is to analyze market data and predict the next trading day stock price.\\nUse historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\\nEnsure that your predictions are well-justified, considering multiple financial factors.\\n\\n\\u2022 Predicted Stock Price: The forecasted close price for the next trading day.\\n\\u2022 Price Movement Likelihood: The likelihood of the predicted stock price.\\n\\u2022 Justification: Provide an explanation for the predicted stock price and the corresponding likelihood, considering the following:\\n  - Historical market data (e.g., recent closing prices).\\n  - Technical indicators (e.g., SMA, EMA...\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: Q&A CHAIN GENERATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " SYSTEM PROMPT:\n",
      "You are a financial analyst who helps explain stock predictions through Q&A.\n",
      "\n",
      " USER PROMPT:\n",
      "You are a financial analyst with expertise in stock market forecasting.\n",
      "Your task is to analyze market data and predict the next trading day stock price.\n",
      "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
      "Ensure that your predictions are well-justified, considering multiple financial factors.\n",
      "\n",
      "‚Ä¢ Predicted Stock Price: The forecasted close price for the next trading day.\n",
      "‚Ä¢ Price Movement Likelihood: The likelihood of the predicted stock price.\n",
      "‚Ä¢ Justification: Provide an explanation for the predicted stock price and the corresponding likelihood, considering the following:\n",
      "  - Historical market data (e.g., recent closing prices).\n",
      "  - Technical indicators (e.g., SMA, EMA, RSI, MACD, Bollinger Bands).\n",
      "  - Sentiment analysis (e.g., news sentiment, market sentiment).\n",
      "\n",
      "Please weigh these signals and justify the predicted stock price.\n",
      "\n",
      "TICKER: HSBC\n",
      "DATE: 2023-01-03\n",
      "\n",
      "RECENT CLOSING PRICES (most recent last): 31.0700, 31.0300, 31.2100, 31.1600, 31.6300\n",
      "\n",
      "TECHNICAL INDICATORS:\n",
      "SMA_20=30.614999866485597, SMA_50=29.027199935913085,\n",
      "EMA_12=30.909996996764857, EMA_26=30.33953977919106,\n",
      "RSI_14=70.01903430263613, MACD=0.5704572175737965, MACD_signal=0.5609859479794548, MACD_hist=0.0094712695943417,\n",
      "BB_width_20_2=0.0625429400529223\n",
      "\n",
      "SENTIMENT AGGREGATES:\n",
      "headline_count=4.0, sent_compound_mean=0.072325\n",
      "\n",
      "HEADLINES (concise):\n",
      "BP, Unilever, and HSBC have failed to properly exit Russia after Ukraine war, new report warns - City AM | Rising interest rates to boost HSBC profit amid uncertain economic outlook - South China Morning Post | HSBC connects virtual and real world with seasonal campaign - Marketing-Interactive | John Lloyd leaves HSBC - Asset Servicing Times\n",
      "\n",
      "Return STRICT JSON with keys:\n",
      "- predicted_close (float, next-day close price),\n",
      "- likelihood (float in [0,1]),\n",
      "- justification (string, 1‚Äì2 sentences).\n",
      "JSON:\n",
      "\n",
      "The predicted answer is: {\"predicted_close\": 32.68000030517578, \"likelihood\": 0.9, \"justification\": \"n/a\"}\n",
      "\n",
      "Generate 3-4 question-answer pairs that help explain the reasoning behind this prediction based on the market data provided. Return as JSON with format: {\"qa_pairs\": [{\"question\": \"...\", \"answer\": \"...\"}]}\n",
      "\n",
      " FULL API REQUEST BODY:\n",
      "{\n",
      "  \"model\": \"gpt-4o-mini\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a financial analyst who helps explain stock predictions through Q&A.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"You are a financial analyst with expertise in stock market forecasting.\\nYour task is to analyze market data and predict the next trading day stock price.\\nUse historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\\nEnsure that your predictions are well-justified, considering multiple financial factors.\\n\\n\\u2022 Predicted Stock Price: The forecasted close price for the next trading day.\\n\\u2022 Price Movement Likelihood: The likelihood of the predicted stock price.\\n\\u2022 Justification: Provide an explanation for the predicted stock price and the corresponding likelihood, considering the following:\\n  - Historical market data (e.g., recent closing prices).\\n  - Technical indicators (e.g., SMA, EMA, RSI, MACD, Bollinge...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate sample prompts from test data\n",
    "print(\" Generating Sample Prompts for GPT-4o\")\n",
    "print(\"=\"*80)\n",
    "generate_sample_prompts(test_data, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3daf71",
   "metadata": {},
   "source": [
    "## 4. Utility: Check Batch Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ace0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 existing batches:\n",
      "\n",
      "test_qa_chain_2: completed - BatchRequestCounts(completed=477, failed=0, total=477)\n",
      "test_qa_chain_1: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "test_qa_chain_0: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "test_justification_2: completed - BatchRequestCounts(completed=477, failed=0, total=477)\n",
      "test_justification_1: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "test_justification_0: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "val_qa_chain_1: completed - BatchRequestCounts(completed=243, failed=0, total=243)\n",
      "val_qa_chain_0: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "val_justification_1: completed - BatchRequestCounts(completed=243, failed=0, total=243)\n",
      "val_justification_0: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_8: completed - BatchRequestCounts(completed=698, failed=0, total=698)\n",
      "train_qa_chain_7: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_6: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_5: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_4: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_3: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_2: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_1: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_qa_chain_0: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_8: completed - BatchRequestCounts(completed=698, failed=0, total=698)\n",
      "train_justification_7: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_6: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_5: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_4: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_3: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_2: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_1: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "train_justification_0: completed - BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "val_justification: completed - BatchRequestCounts(completed=1243, failed=0, total=1243)\n",
      "train_qa_chain: failed - BatchRequestCounts(completed=0, failed=0, total=0)\n",
      "train_justification: failed - BatchRequestCounts(completed=0, failed=0, total=0)\n"
     ]
    }
   ],
   "source": [
    "# List all existing batches\n",
    "existing = retrieve_existing_batches()\n",
    "print(f\"Found {len(existing)} existing batches:\\n\")\n",
    "for desc, info in existing.items():\n",
    "    print(f\"{desc}: {info['status']} - {info['request_counts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6918273",
   "metadata": {},
   "source": [
    "## 5A. RECOMMENDED: Submit All Batches (Async - Non-blocking)\n",
    "\n",
    "**Use this approach!** Submit all batches at once and check back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db58981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Submitting TRAIN batches\n",
      "============================================================\n",
      "\n",
      "--- Task: justification ---\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk1.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk2.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk3.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk4.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk5.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk6.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk7.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk8.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_justification_batch_chunk9.jsonl with 698 requests\n",
      "‚úì Batch 'train_justification_0' already exists: completed\n",
      "‚úì Batch 'train_justification_1' already exists: completed\n",
      "‚úì Batch 'train_justification_2' already exists: completed\n",
      "‚úì Batch 'train_justification_3' already exists: completed\n",
      "‚úì Batch 'train_justification_4' already exists: completed\n",
      "‚úì Batch 'train_justification_5' already exists: completed\n",
      "‚úì Batch 'train_justification_6' already exists: completed\n",
      "‚úì Batch 'train_justification_7' already exists: completed\n",
      "‚úì Batch 'train_justification_8' already exists: completed\n",
      "\n",
      "--- Task: qa_chain ---\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk1.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk2.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk3.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk4.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk5.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk6.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk7.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk8.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/train_qa_chain_batch_chunk9.jsonl with 698 requests\n",
      "‚úì Batch 'train_qa_chain_0' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_1' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_2' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_3' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_4' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_5' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_6' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_7' already exists: completed\n",
      "‚úì Batch 'train_qa_chain_8' already exists: completed\n",
      "\n",
      "============================================================\n",
      "Submitting VAL batches\n",
      "============================================================\n",
      "\n",
      "--- Task: justification ---\n",
      "Created batch file: batch_inputs/val_justification_batch_chunk1.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/val_justification_batch_chunk2.jsonl with 243 requests\n",
      "‚úì Batch 'val_justification_0' already exists: completed\n",
      "‚úì Batch 'val_justification_1' already exists: completed\n",
      "\n",
      "--- Task: qa_chain ---\n",
      "Created batch file: batch_inputs/val_qa_chain_batch_chunk1.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/val_qa_chain_batch_chunk2.jsonl with 243 requests\n",
      "‚úì Batch 'val_qa_chain_0' already exists: completed\n",
      "‚úì Batch 'val_qa_chain_1' already exists: completed\n",
      "\n",
      "============================================================\n",
      "Submitting TEST batches\n",
      "============================================================\n",
      "\n",
      "--- Task: justification ---\n",
      "Created batch file: batch_inputs/test_justification_batch_chunk1.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/test_justification_batch_chunk2.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/test_justification_batch_chunk3.jsonl with 477 requests\n",
      "‚úì Batch 'test_justification_0' already exists: completed\n",
      "‚úì Batch 'test_justification_1' already exists: completed\n",
      "‚úì Batch 'test_justification_2' already exists: completed\n",
      "\n",
      "--- Task: qa_chain ---\n",
      "Created batch file: batch_inputs/test_qa_chain_batch_chunk1.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/test_qa_chain_batch_chunk2.jsonl with 1000 requests\n",
      "Created batch file: batch_inputs/test_qa_chain_batch_chunk3.jsonl with 477 requests\n",
      "‚úì Batch 'test_qa_chain_0' already exists: completed\n",
      "‚úì Batch 'test_qa_chain_1' already exists: completed\n",
      "‚úì Batch 'test_qa_chain_2' already exists: completed\n",
      "\n",
      "============================================================\n",
      "‚úì Submitted 28 batch jobs!\n",
      "============================================================\n",
      "\n",
      "Batch IDs:\n",
      "  train_justification_0: batch_68f52bdfd60c819085cb871807820a81\n",
      "  train_justification_1: batch_68f52cd8bd74819092494bde27a8ebb4\n",
      "  train_justification_2: batch_68f531075cb4819085cda6749b01469e\n",
      "  train_justification_3: batch_68f535547d0081909bb6b92bc2cc2189\n",
      "  train_justification_4: batch_68f536a993208190a2dfdaf1fd7a14ba\n",
      "  train_justification_5: batch_68f53c0931408190bcb8578144a8026f\n",
      "  train_justification_6: batch_68f5401a68108190aacc79a9e820623d\n",
      "  train_justification_7: batch_68f540d6dbac8190bb1b9684c776c2dc\n",
      "  train_justification_8: batch_68f544e631008190ab353b6e1918bb65\n",
      "  train_qa_chain_0: batch_68f5491397a881909fce3040713d387c\n",
      "  train_qa_chain_1: batch_68f5ac6ba1548190a5b37f4f60f7a96b\n",
      "  train_qa_chain_2: batch_68f5afaa03408190b194efb94d92706b\n",
      "  train_qa_chain_3: batch_68f5b17a42188190b8962b8c80310c3c\n",
      "  train_qa_chain_4: batch_68f5b84eb5888190908f3b196e4cde77\n",
      "  train_qa_chain_5: batch_68f5bc80304081908114ded2c3b4f485\n",
      "  train_qa_chain_6: batch_68f5bdd691688190889c97970001028a\n",
      "  train_qa_chain_7: batch_68f5c2624bd08190a2ad934459d8f90e\n",
      "  train_qa_chain_8: batch_68f5c6aef3988190a21a2f2e2a73c7ac\n",
      "  val_justification_0: batch_68f5cb2b9a988190bf4aa5d1a6e28575\n",
      "  val_justification_1: batch_68f5cbcb0ff481908606ab8b8ea4f5d4\n",
      "  val_qa_chain_0: batch_68f5ccc4b63c81909a249821353d83da\n",
      "  val_qa_chain_1: batch_68f5d09ddd08819096e52d8f614501ad\n",
      "  test_justification_0: batch_68f5d15a0d3c81908ed7ff8834d3f64e\n",
      "  test_justification_1: batch_68f5d56c963081908b018e592ee65f42\n",
      "  test_justification_2: batch_68f5d62886f88190a124c1789a82e2c3\n",
      "  test_qa_chain_0: batch_68f5da051d64819088895d7c9f45ba29\n",
      "  test_qa_chain_1: batch_68f5dae0666081908d256530bbca1d44\n",
      "  test_qa_chain_2: batch_68f5df11a74c8190bc6875f14609bd88\n"
     ]
    }
   ],
   "source": [
    "def submit_all_batches(chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Submit all batch jobs without waiting for completion\n",
    "    Returns dict of submitted batch IDs organized by dataset/task/chunk\n",
    "    \"\"\"\n",
    "    submitted_batches = {}\n",
    "    existing_batches = retrieve_existing_batches()\n",
    "    \n",
    "    datasets = {\n",
    "        'train': train_data,\n",
    "        'val': val_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "    \n",
    "    for dataset_name, data in datasets.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Submitting {dataset_name.upper()} batches\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for task_type in ['justification', 'qa_chain']:\n",
    "            print(f\"\\n--- Task: {task_type} ---\")\n",
    "            \n",
    "            # Prepare batch files\n",
    "            batch_file_base = os.path.join(BATCH_INPUT_DIR, f\"{dataset_name}_{task_type}_batch.jsonl\")\n",
    "            batch_files = prepare_batch_file(data, task_type, batch_file_base, chunk_size=chunk_size)\n",
    "            \n",
    "            # Submit each chunk\n",
    "            for chunk_idx, batch_file in enumerate(batch_files):\n",
    "                batch_desc = f\"{dataset_name}_{task_type}_{chunk_idx}\"\n",
    "                \n",
    "                # Check if already exists\n",
    "                if batch_desc in existing_batches:\n",
    "                    existing = existing_batches[batch_desc]\n",
    "                    print(f\"‚úì Batch '{batch_desc}' already exists: {existing['status']}\")\n",
    "                    submitted_batches[batch_desc] = existing['id']\n",
    "                    continue\n",
    "                \n",
    "                # Upload and submit new batch\n",
    "                file_id = upload_file_and_wait(batch_file)\n",
    "                \n",
    "                print(f\"Submitting batch job: {batch_desc}\")\n",
    "                batch = client.batches.create(\n",
    "                    input_file_id=file_id,\n",
    "                    endpoint=\"/v1/chat/completions\",\n",
    "                    completion_window=\"24h\",\n",
    "                    metadata={\"description\": batch_desc}\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úì Submitted batch ID: {batch.id} - Status: {batch.status}\")\n",
    "                submitted_batches[batch_desc] = batch.id\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Submitted {len(submitted_batches)} batch jobs!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nBatch IDs:\")\n",
    "    for desc, batch_id in submitted_batches.items():\n",
    "        print(f\"  {desc}: {batch_id}\")\n",
    "    \n",
    "    return submitted_batches\n",
    "\n",
    "# Submit all batches\n",
    "submitted_batches = submit_all_batches(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d288",
   "metadata": {},
   "source": [
    "## 5B. Check Status of All Batches\n",
    "\n",
    "Run this cell periodically to check progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25eb5b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Status Summary:\n",
      "================================================================================\n",
      "\n",
      "test_qa_chain_2\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=477, failed=0, total=477)\n",
      "\n",
      "test_qa_chain_1\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "test_qa_chain_0\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "test_justification_2\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=477, failed=0, total=477)\n",
      "\n",
      "test_justification_1\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "test_justification_0\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "val_qa_chain_1\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=243, failed=0, total=243)\n",
      "\n",
      "val_qa_chain_0\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "val_justification_1\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=243, failed=0, total=243)\n",
      "\n",
      "val_justification_0\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_8\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=698, failed=0, total=698)\n",
      "\n",
      "train_qa_chain_7\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_6\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_5\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_4\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_3\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_2\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_1\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_qa_chain_0\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_8\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=698, failed=0, total=698)\n",
      "\n",
      "train_justification_7\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_6\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_5\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_4\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_3\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_2\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_1\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "train_justification_0\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1000, failed=0, total=1000)\n",
      "\n",
      "val_justification\n",
      "  Status: completed\n",
      "  Counts: BatchRequestCounts(completed=1243, failed=0, total=1243)\n",
      "\n",
      "train_qa_chain\n",
      "  Status: failed\n",
      "  Counts: BatchRequestCounts(completed=0, failed=0, total=0)\n",
      "\n",
      "train_justification\n",
      "  Status: failed\n",
      "  Counts: BatchRequestCounts(completed=0, failed=0, total=0)\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "  ‚úì Completed: 29\n",
      "  ‚è≥ In Progress: 0\n",
      "  üîÑ Validating: 0\n",
      "  ‚úó Failed: 2\n",
      "  ? Other: 0\n"
     ]
    }
   ],
   "source": [
    "def check_all_batches_status():\n",
    "    \"\"\"Check status of all batches\"\"\"\n",
    "    existing = retrieve_existing_batches()\n",
    "    \n",
    "    status_summary = {\n",
    "        'completed': [],\n",
    "        'in_progress': [],\n",
    "        'validating': [],\n",
    "        'failed': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    print(\"Batch Status Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for desc, info in existing.items():\n",
    "        status = info['status']\n",
    "        if status == 'completed':\n",
    "            status_summary['completed'].append(desc)\n",
    "        elif status in ['in_progress', 'finalizing']:\n",
    "            status_summary['in_progress'].append(desc)\n",
    "        elif status == 'validating':\n",
    "            status_summary['validating'].append(desc)\n",
    "        elif status == 'failed':\n",
    "            status_summary['failed'].append(desc)\n",
    "        else:\n",
    "            status_summary['other'].append(desc)\n",
    "        \n",
    "        print(f\"\\n{desc}\")\n",
    "        print(f\"  Status: {status}\")\n",
    "        print(f\"  Counts: {info['request_counts']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Summary:\")\n",
    "    print(f\"  ‚úì Completed: {len(status_summary['completed'])}\")\n",
    "    print(f\"  ‚è≥ In Progress: {len(status_summary['in_progress'])}\")\n",
    "    print(f\"  üîÑ Validating: {len(status_summary['validating'])}\")\n",
    "    print(f\"  ‚úó Failed: {len(status_summary['failed'])}\")\n",
    "    print(f\"  ? Other: {len(status_summary['other'])}\")\n",
    "    \n",
    "    return status_summary\n",
    "\n",
    "# Check status\n",
    "status = check_all_batches_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d8c8c",
   "metadata": {},
   "source": [
    "## 5C. Retrieve All Completed Results\n",
    "\n",
    "Run this once all batches are completed to download results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fad656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Retrieving TRAIN results\n",
      "============================================================\n",
      "\n",
      "--- Task: justification ---\n",
      "‚úì Retrieving chunk 1/9 from file-Ciu5etNRMzs1cMNSnAjhfD\n",
      "\n",
      "Downloading results from file-Ciu5etNRMzs1cMNSnAjhfD...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 2/9 from file-KHVjZWSgmdfrREWDvfSuxs\n",
      "\n",
      "Downloading results from file-KHVjZWSgmdfrREWDvfSuxs...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 3/9 from file-1SuuHMQk2zusEBxHCw4Drq\n",
      "\n",
      "Downloading results from file-1SuuHMQk2zusEBxHCw4Drq...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 4/9 from file-2wwTfNDBqJnjWPiEdvPXbq\n",
      "\n",
      "Downloading results from file-2wwTfNDBqJnjWPiEdvPXbq...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 5/9 from file-1WDW2bfe33chEUoAAFNg8B\n",
      "\n",
      "Downloading results from file-1WDW2bfe33chEUoAAFNg8B...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 6/9 from file-WqxGSTtJ99v1KvzPH5nxxB\n",
      "\n",
      "Downloading results from file-WqxGSTtJ99v1KvzPH5nxxB...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 7/9 from file-AvtZBKssNNCqSZsJbz23Mq\n",
      "\n",
      "Downloading results from file-AvtZBKssNNCqSZsJbz23Mq...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 8/9 from file-AeQjJT3N3kiUVBWqTsuxMp\n",
      "\n",
      "Downloading results from file-AeQjJT3N3kiUVBWqTsuxMp...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 9/9 from file-9fTs716sa7Enp2bgNso5Sg\n",
      "\n",
      "Downloading results from file-9fTs716sa7Enp2bgNso5Sg...\n",
      "Processed 698 results\n",
      "‚úì Retrieved 8698 samples for train_justification\n",
      "\n",
      "--- Task: qa_chain ---\n",
      "‚úì Retrieving chunk 1/9 from file-WbivFrJEWoyPqZNV7HaQUS\n",
      "\n",
      "Downloading results from file-WbivFrJEWoyPqZNV7HaQUS...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 2/9 from file-AzFhVH65YEJb13ZHTQFjgy\n",
      "\n",
      "Downloading results from file-AzFhVH65YEJb13ZHTQFjgy...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 3/9 from file-MhXixLKFiZzAVjRArgGbyB\n",
      "\n",
      "Downloading results from file-MhXixLKFiZzAVjRArgGbyB...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 4/9 from file-Y9caPTzCfKf5JiLuEkqYBg\n",
      "\n",
      "Downloading results from file-Y9caPTzCfKf5JiLuEkqYBg...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 5/9 from file-Qn63nuNiY87hJQFfD9LwQq\n",
      "\n",
      "Downloading results from file-Qn63nuNiY87hJQFfD9LwQq...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 6/9 from file-ErsgMD6eLoiC4YWk8K1dmh\n",
      "\n",
      "Downloading results from file-ErsgMD6eLoiC4YWk8K1dmh...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 7/9 from file-3xwhJ4QFivoBq2w8kLnqzR\n",
      "\n",
      "Downloading results from file-3xwhJ4QFivoBq2w8kLnqzR...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 8/9 from file-68UQq5VsbpV31awqhr9WRW\n",
      "\n",
      "Downloading results from file-68UQq5VsbpV31awqhr9WRW...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 9/9 from file-PbLcYtGDkjjYCcP741MZw7\n",
      "\n",
      "Downloading results from file-PbLcYtGDkjjYCcP741MZw7...\n",
      "Processed 698 results\n",
      "‚úì Retrieved 8698 samples for train_qa_chain\n",
      "\n",
      "============================================================\n",
      "Retrieving VAL results\n",
      "============================================================\n",
      "\n",
      "--- Task: justification ---\n",
      "‚úì Retrieving chunk 1/2 from file-LERTCBpEcy5QMCEPP9z5WK\n",
      "\n",
      "Downloading results from file-LERTCBpEcy5QMCEPP9z5WK...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 2/2 from file-E4voAviNcV4moH2CjYFw57\n",
      "\n",
      "Downloading results from file-E4voAviNcV4moH2CjYFw57...\n",
      "Processed 243 results\n",
      "‚úì Retrieved 1243 samples for val_justification\n",
      "\n",
      "--- Task: qa_chain ---\n",
      "‚úì Retrieving chunk 1/2 from file-8a8TMsxr5FbM8JcrhCVQwx\n",
      "\n",
      "Downloading results from file-8a8TMsxr5FbM8JcrhCVQwx...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 2/2 from file-HpUdMuy234WdBTkHWJdkdh\n",
      "\n",
      "Downloading results from file-HpUdMuy234WdBTkHWJdkdh...\n",
      "Processed 243 results\n",
      "‚úì Retrieved 1243 samples for val_qa_chain\n",
      "\n",
      "============================================================\n",
      "Retrieving TEST results\n",
      "============================================================\n",
      "\n",
      "--- Task: justification ---\n",
      "‚úì Retrieving chunk 1/3 from file-5LGcM61HhzsJmfa4KTqL5E\n",
      "\n",
      "Downloading results from file-5LGcM61HhzsJmfa4KTqL5E...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 2/3 from file-FVNtcJCDz8VR1xxMq8Bc4k\n",
      "\n",
      "Downloading results from file-FVNtcJCDz8VR1xxMq8Bc4k...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 3/3 from file-UdnACSMg5eLvgRuEV4VneJ\n",
      "\n",
      "Downloading results from file-UdnACSMg5eLvgRuEV4VneJ...\n",
      "Processed 477 results\n",
      "‚úì Retrieved 2477 samples for test_justification\n",
      "\n",
      "--- Task: qa_chain ---\n",
      "‚úì Retrieving chunk 1/3 from file-4qQ8Hm6ojmLexLhAbtjouC\n",
      "\n",
      "Downloading results from file-4qQ8Hm6ojmLexLhAbtjouC...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 2/3 from file-PbKEei6SWB4Sr65ia9Hanz\n",
      "\n",
      "Downloading results from file-PbKEei6SWB4Sr65ia9Hanz...\n",
      "Processed 1000 results\n",
      "‚úì Retrieving chunk 3/3 from file-GXbqxmu28ddsoKRwMSz6Fm\n",
      "\n",
      "Downloading results from file-GXbqxmu28ddsoKRwMSz6Fm...\n",
      "Processed 477 results\n",
      "‚úì Retrieved 2477 samples for test_qa_chain\n",
      "\n",
      "============================================================\n",
      "‚úì Retrieval complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def retrieve_all_results(chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Retrieve results from all completed batches\n",
    "    Returns dict of augmented data organized by dataset and task\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    existing_batches = retrieve_existing_batches()\n",
    "    \n",
    "    datasets = {\n",
    "        'train': train_data,\n",
    "        'val': val_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "    \n",
    "    for dataset_name, data in datasets.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Retrieving {dataset_name.upper()} results\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for task_type in ['justification', 'qa_chain']:\n",
    "            print(f\"\\n--- Task: {task_type} ---\")\n",
    "            \n",
    "            # Calculate number of chunks\n",
    "            num_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "            \n",
    "            all_augmented = []\n",
    "            for chunk_idx in range(num_chunks):\n",
    "                batch_desc = f\"{dataset_name}_{task_type}_{chunk_idx}\"\n",
    "                \n",
    "                if batch_desc not in existing_batches:\n",
    "                    print(f\"‚ö† Warning: Batch '{batch_desc}' not found!\")\n",
    "                    continue\n",
    "                \n",
    "                batch_info = existing_batches[batch_desc]\n",
    "                \n",
    "                if batch_info['status'] != 'completed':\n",
    "                    print(f\"‚ö† Batch '{batch_desc}' not completed yet (status: {batch_info['status']})\")\n",
    "                    continue\n",
    "                \n",
    "                if not batch_info['output_file_id']:\n",
    "                    print(f\"‚ö† Batch '{batch_desc}' has no output file!\")\n",
    "                    continue\n",
    "                \n",
    "                # Get data slice for this chunk\n",
    "                start_idx = chunk_idx * chunk_size\n",
    "                end_idx = min(start_idx + chunk_size, len(data))\n",
    "                data_slice = data[start_idx:end_idx]\n",
    "                \n",
    "                # Process results\n",
    "                print(f\"‚úì Retrieving chunk {chunk_idx+1}/{num_chunks} from {batch_info['output_file_id']}\")\n",
    "                augmented_chunk = process_results(batch_info['output_file_id'], data_slice)\n",
    "                all_augmented.extend(augmented_chunk)\n",
    "            \n",
    "            # Store results\n",
    "            if all_augmented:\n",
    "                results[f\"{dataset_name}_{task_type}\"] = all_augmented\n",
    "                print(f\"‚úì Retrieved {len(all_augmented)} samples for {dataset_name}_{task_type}\")\n",
    "            else:\n",
    "                print(f\"‚úó No results retrieved for {dataset_name}_{task_type}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì Retrieval complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Retrieve all completed results\n",
    "results = retrieve_all_results(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78421f1c",
   "metadata": {},
   "source": [
    "## 6. Save Augmented Data\n",
    "\n",
    "Save the final datasets in multiple formats for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f95955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving train data...\n",
      "  ‚úì Saved with justifications: ../finetune_paper/train_with_justifications.jsonl\n",
      "  ‚úì Saved with Q&A: ../finetune_paper/train_with_qa.jsonl\n",
      "\n",
      "Saving val data...\n",
      "  ‚úì Saved with justifications: ../finetune_paper/val_with_justifications.jsonl\n",
      "  ‚úì Saved with Q&A: ../finetune_paper/val_with_qa.jsonl\n",
      "\n",
      "Saving test data...\n",
      "  ‚úì Saved with justifications: ../finetune_paper/test_with_justifications.jsonl\n",
      "  ‚úì Saved with Q&A: ../finetune_paper/test_with_qa.jsonl\n",
      "\n",
      "‚úì All data saved successfully!\n",
      "  ‚úì Saved with Q&A: ../finetune_paper/train_with_qa.jsonl\n",
      "\n",
      "Saving val data...\n",
      "  ‚úì Saved with justifications: ../finetune_paper/val_with_justifications.jsonl\n",
      "  ‚úì Saved with Q&A: ../finetune_paper/val_with_qa.jsonl\n",
      "\n",
      "Saving test data...\n",
      "  ‚úì Saved with justifications: ../finetune_paper/test_with_justifications.jsonl\n",
      "  ‚úì Saved with Q&A: ../finetune_paper/test_with_qa.jsonl\n",
      "\n",
      "‚úì All data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_augmented_data(results):\n",
    "    \"\"\"Save results in various formats for fine-tuning\"\"\"\n",
    "    \n",
    "    for dataset_name in ['train', 'val', 'test']:\n",
    "        print(f\"\\nSaving {dataset_name} data...\")\n",
    "        \n",
    "        just_key = f\"{dataset_name}_justification\"\n",
    "        qa_key = f\"{dataset_name}_qa_chain\"\n",
    "        \n",
    "        if just_key not in results or qa_key not in results:\n",
    "            print(f\"Missing results for {dataset_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get the data\n",
    "        just_data = results[just_key]\n",
    "        qa_data = results[qa_key]\n",
    "        \n",
    "        # Format 1: With justification - use LLM response directly\n",
    "        finetuning_just = []\n",
    "        for sample in just_data:\n",
    "            ft_sample = {\n",
    "                \"prompt\": sample[\"prompt\"],\n",
    "                \"response\": sample['generated_content']\n",
    "            }\n",
    "            finetuning_just.append(ft_sample)\n",
    "        \n",
    "        output_file = f\"../finetune_paper/{dataset_name}_with_justifications.jsonl\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            for sample in finetuning_just:\n",
    "                f.write(json.dumps(sample) + '\\n')\n",
    "        print(f\"  ‚úì Saved with justifications: {output_file}\")\n",
    "        \n",
    "        # Format 2: With Q&A - create full response with predicted_close, likelihood, and Q&A in justification\n",
    "        finetuning_qa = []\n",
    "        for sample in qa_data:\n",
    "            # Parse the original response to get predicted_close and likelihood\n",
    "            try:\n",
    "                original_response = json.loads(sample['response'])\n",
    "                predicted_close = original_response.get('predicted_close', 0.0)\n",
    "                likelihood = original_response.get('likelihood', 0.5)\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                predicted_close = 0.0\n",
    "                likelihood = 0.5\n",
    "            \n",
    "            # Parse the Q&A content and extract the array\n",
    "            try:\n",
    "                qa_content = json.loads(sample['generated_content'])\n",
    "                if 'qa_pairs' in qa_content:\n",
    "                    # Extract just the qa_pairs array\n",
    "                    qa_array = qa_content['qa_pairs']\n",
    "                else:\n",
    "                    qa_array = []\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                qa_array = []\n",
    "            \n",
    "            # Build complete response JSON with Q&A in justification field\n",
    "            complete_response = {\n",
    "                \"predicted_close\": predicted_close,\n",
    "                \"likelihood\": likelihood,\n",
    "                \"justification\": qa_array\n",
    "            }\n",
    "            \n",
    "            ft_sample = {\n",
    "                \"prompt\": sample[\"prompt\"],\n",
    "                \"response\": json.dumps(complete_response)\n",
    "            }\n",
    "            finetuning_qa.append(ft_sample)\n",
    "        \n",
    "        output_file = f\"../finetune_paper/{dataset_name}_with_qa.jsonl\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            for sample in finetuning_qa:\n",
    "                f.write(json.dumps(sample) + '\\n')\n",
    "        print(f\"  ‚úì Saved with Q&A: {output_file}\")\n",
    "\n",
    "# Save all the data\n",
    "save_augmented_data(results)\n",
    "print(\"\\n‚úì All data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583f7cc",
   "metadata": {},
   "source": [
    "## 7. Convert to Instruction/Input/Output Format\n",
    "\n",
    "Convert the data into instruction/input/output format for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff15a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datasets to instruction/input/output format...\n",
      "============================================================\n",
      "\n",
      "TRAIN dataset:\n",
      "‚úì Converted 8698 samples\n",
      "‚úì Saved to: ../finetune_paper/train_instruction_format.jsonl\n",
      "‚úì Converted 8698 samples\n",
      "‚úì Saved to: ../finetune_paper/train_with_justifications_instruction_format.jsonl\n",
      "‚úì Converted 8698 samples\n",
      "‚úì Saved to: ../finetune_paper/train_with_qa_instruction_format.jsonl\n",
      "\n",
      "VAL dataset:\n",
      "‚úì Converted 1243 samples\n",
      "‚úì Saved to: ../finetune_paper/val_instruction_format.jsonl\n",
      "‚úì Converted 1243 samples\n",
      "‚úì Saved to: ../finetune_paper/val_with_justifications_instruction_format.jsonl\n",
      "‚úì Converted 1243 samples\n",
      "‚úì Saved to: ../finetune_paper/val_with_qa_instruction_format.jsonl\n",
      "\n",
      "TEST dataset:\n",
      "‚úì Converted 2477 samples\n",
      "‚úì Saved to: ../finetune_paper/test_instruction_format.jsonl\n",
      "‚úì Converted 2477 samples\n",
      "‚úì Saved to: ../finetune_paper/test_with_justifications_instruction_format.jsonl\n",
      "‚úì Converted 2477 samples\n",
      "‚úì Saved to: ../finetune_paper/test_with_qa_instruction_format.jsonl\n",
      "\n",
      "============================================================\n",
      "‚úì All conversions complete!\n"
     ]
    }
   ],
   "source": [
    "def convert_to_instruction_input_output(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert prompt/response format to instruction/input/output format\n",
    "    \n",
    "    instruction: The system prompt (general task description)\n",
    "    input: The specific data for this prediction (ticker, date, prices, indicators, etc.)\n",
    "    output: The response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the instruction (the general task without specific data)\n",
    "    instruction = \"\"\"You are a financial analyst with expertise in stock market forecasting.\n",
    "Your task is to analyze market data and predict the next trading day stock price.\n",
    "Use historical price trends, technical indicators, and sentiment analysis to provide an informed forecast.\n",
    "Ensure that your predictions are well-justified, considering multiple financial factors.\n",
    "\n",
    "‚Ä¢ Predicted Stock Price: The forecasted close price for the next trading day.\n",
    "‚Ä¢ Price Movement Likelihood: The likelihood of the predicted stock price.\n",
    "‚Ä¢ Justification: Provide an explanation for the predicted stock price and the corresponding likelihood, considering the following:\n",
    "  - Historical market data (e.g., recent closing prices).\n",
    "  - Technical indicators (e.g., SMA, EMA, RSI, MACD, Bollinger Bands).\n",
    "  - Sentiment analysis (e.g., news sentiment, market sentiment).\n",
    "\n",
    "Please weigh these signals and justify the predicted stock price.\n",
    "\n",
    "Return STRICT JSON with keys:\n",
    "- predicted_close (float, next-day close price),\n",
    "- likelihood (float in [0,1]),\n",
    "- justification (string, 1‚Äì2 sentences).\"\"\"\n",
    "    \n",
    "    converted_data = []\n",
    "    \n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            prompt = sample['prompt']\n",
    "            response = sample['response']\n",
    "            \n",
    "            # Extract the input part (everything after \"Please weigh these signals...\")\n",
    "            # Find where the specific data starts (after the instruction part)\n",
    "            if \"TICKER:\" in prompt:\n",
    "                # Split at TICKER to separate instruction from input\n",
    "                parts = prompt.split(\"TICKER:\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    # Extract just the data portion\n",
    "                    input_data = \"TICKER:\" + parts[1].strip()\n",
    "                    \n",
    "                    # Remove the \"Return STRICT JSON...\" part from input if it exists\n",
    "                    if \"Return STRICT JSON\" in input_data:\n",
    "                        input_data = input_data.split(\"Return STRICT JSON\")[0].strip()\n",
    "                    \n",
    "                    converted_sample = {\n",
    "                        \"instruction\": instruction,\n",
    "                        \"input\": input_data,\n",
    "                        \"output\": response\n",
    "                    }\n",
    "                    converted_data.append(converted_sample)\n",
    "    \n",
    "    # Save converted data\n",
    "    with open(output_file, 'w') as f:\n",
    "        for sample in converted_data:\n",
    "            f.write(json.dumps(sample) + '\\n')\n",
    "    \n",
    "    print(f\"‚úì Converted {len(converted_data)} samples\")\n",
    "    print(f\"‚úì Saved to: {output_file}\")\n",
    "    return converted_data\n",
    "\n",
    "\n",
    "# Convert all datasets\n",
    "print(\"Converting datasets to instruction/input/output format...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dataset_name in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{dataset_name.upper()} dataset:\")\n",
    "    \n",
    "    # Convert base dataset\n",
    "    convert_to_instruction_input_output(\n",
    "        f\"../finetune_paper/{dataset_name}.jsonl\",\n",
    "        f\"../finetune_paper/{dataset_name}_instruction_format.jsonl\"\n",
    "    )\n",
    "    \n",
    "    # Convert with justifications\n",
    "    convert_to_instruction_input_output(\n",
    "        f\"../finetune_paper/{dataset_name}_with_justifications.jsonl\",\n",
    "        f\"../finetune_paper/{dataset_name}_with_justifications_instruction_format.jsonl\"\n",
    "    )\n",
    "    \n",
    "    # Convert with Q&A\n",
    "    convert_to_instruction_input_output(\n",
    "        f\"../finetune_paper/{dataset_name}_with_qa.jsonl\",\n",
    "        f\"../finetune_paper/{dataset_name}_with_qa_instruction_format.jsonl\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì All conversions complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
